{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f57228f",
   "metadata": {},
   "source": [
    "# (Legacy) LSTM_fixed.ipynb\n",
    "\n",
    "此 notebook 的通用组件（数据/特征/序列化/训练/回测）已模块化到 `fyp_trading/`。\n",
    "\n",
    "- 建议：如需复现实验，请改用模块化脚本；此处仅保留作历史参考。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Fixed-window LSTM (PyTorch) for time-series modeling\n",
    "#\n",
    "# This notebook converts the previous walk-forward (expanding) training scheme\n",
    "# into a strict fixed-window scheme:\n",
    "# - For each fold:\n",
    "#   - Train on the most recent `train_window` sequences,\n",
    "#   - Validate on the subsequent `val_size` sequences,\n",
    "#   - Test on the subsequent `test_size` sequences.\n",
    "# - Step forward by `step_size` (default = `test_size`) for the next fold.\n",
    "#\n",
    "# All other components (feature engineering, sequence building, LSTM models,\n",
    "# standardization only using the training set, evaluation, visualization, OOF)\n",
    "# remain the same in spirit—only the CV windowing logic is changed to fixed-length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa647fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os, sys, math, random, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid') if 'seaborn-v0_8-whitegrid' in plt.style.available else plt.style.use('ggplot')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, roc_auc_score,\n",
    "    mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# IPython display (fallback to print if not in notebook)\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    def display(x): print(x)\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Device and seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Make CuDNN more reproducible (slightly slower)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10f696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 1) Data fetching\n",
    "# - Ticker: 2800.HK\n",
    "# - Period: 2 years\n",
    "# - Interval: 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b6dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def fetch_prices(ticker=\"2800.HK\", period=\"1y\", interval=\"1d\") -> pd.DataFrame:\n",
    "    tk = yf.Ticker(ticker)\n",
    "    df = tk.history(period=period, interval=interval, auto_adjust=True)\n",
    "    if df is None or df.empty:\n",
    "        end = pd.Timestamp.today().normalize()\n",
    "        start = end - pd.Timedelta(days=3*370)\n",
    "        df = tk.history(start=start, end=end, interval=interval, auto_adjust=True)\n",
    "    if df is None or df.empty:\n",
    "        raise RuntimeError(f\"Cannot fetch data for {ticker}\")\n",
    "    df.index = pd.to_datetime(df.index).tz_localize(None)\n",
    "    return df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].copy()\n",
    "\n",
    "ticker = \"2800.HK\"\n",
    "df_raw = fetch_prices(ticker=ticker, period=\"1y\", interval=\"1d\")\n",
    "print(f\"Data range: {df_raw.index.min().date()} to {df_raw.index.max().date()}\")\n",
    "print(f\"Total samples: {len(df_raw)}\")\n",
    "display(df_raw.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f51b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 2) Feature engineering (aligned with prior logic, no leakage)\n",
    "# - Label: next_return (default log return), classification target_up = (next_return > 0)\n",
    "# - Features: OHLCV-derived, volatility/MAs/RSI/MACD/Bollinger/volume/calendar/lagged returns\n",
    "# - Note: We keep the original EMA-based RSI/MACD and shift(1) usage to avoid leakage,\n",
    "#         and do not alter other logics beyond the CV windowing scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750483df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def calculate_rsi(series: pd.Series, period: int = 14) -> pd.Series:\n",
    "    delta = series.diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -delta.clip(upper=0)\n",
    "    roll_up = up.ewm(alpha=1/period, adjust=False).mean()\n",
    "    roll_down = down.ewm(alpha=1/period, adjust=False).mean().replace(0, np.nan)\n",
    "    rs = roll_up / roll_down\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def calculate_macd(close: pd.Series, fast=12, slow=26, signal=9):\n",
    "    ema_fast = close.ewm(span=fast, adjust=False).mean()\n",
    "    ema_slow = close.ewm(span=slow, adjust=False).mean()\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
    "    histogram = macd_line - signal_line\n",
    "    return macd_line, signal_line, histogram\n",
    "\n",
    "def build_features_and_labels(df: pd.DataFrame,\n",
    "                              horizon: int = 1,\n",
    "                              use_log_return: bool = True):\n",
    "    \"\"\"\n",
    "    horizon: prediction step (days), default 1\n",
    "    use_log_return: True for log returns, False for simple returns\n",
    "    \"\"\"\n",
    "    close = df[\"Close\"].copy()\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Price changes (historical)\n",
    "    features[\"ret_1d\"]  = close.pct_change(1)\n",
    "    features[\"ret_2d\"]  = close.pct_change(2)\n",
    "    features[\"ret_5d\"]  = close.pct_change(5)\n",
    "    features[\"ret_10d\"] = close.pct_change(10)\n",
    "\n",
    "    # Realized volatility\n",
    "    features[\"vol_5d\"]  = features[\"ret_1d\"].rolling(5).std()\n",
    "    features[\"vol_10d\"] = features[\"ret_1d\"].rolling(10).std()\n",
    "    features[\"vol_20d\"] = features[\"ret_1d\"].rolling(20).std()\n",
    "\n",
    "    # Moving averages and relative positions\n",
    "    sma_5  = close.rolling(5).mean()\n",
    "    sma_10 = close.rolling(10).mean()\n",
    "    sma_20 = close.rolling(20).mean()\n",
    "    sma_50 = close.rolling(50).mean()\n",
    "\n",
    "    # Use close.shift(1) when comparing to MAs to avoid leakage\n",
    "    features[\"close_to_sma5\"]  = close.shift(1) / sma_5 - 1\n",
    "    features[\"close_to_sma10\"] = close.shift(1) / sma_10 - 1\n",
    "    features[\"close_to_sma20\"] = close.shift(1) / sma_20 - 1\n",
    "    features[\"sma5_sma10\"]     = sma_5 / sma_10 - 1\n",
    "    features[\"sma10_sma20\"]    = sma_10 / sma_20 - 1\n",
    "\n",
    "    # Technical indicators (use past info)\n",
    "    features[\"rsi_14\"] = calculate_rsi(close.shift(1), 14)\n",
    "    macd_line, signal_line, histogram = calculate_macd(close.shift(1))\n",
    "    features[\"macd\"]        = macd_line\n",
    "    features[\"macd_signal\"] = signal_line\n",
    "    features[\"macd_hist\"]   = histogram\n",
    "\n",
    "    # Bollinger band relative positions\n",
    "    bb_period = 20\n",
    "    bb_std = close.rolling(bb_period).std()\n",
    "    bb_mean = close.rolling(bb_period).mean()\n",
    "    features[\"bb_upper\"] = (bb_mean + 2 * bb_std - close.shift(1)) / close.shift(1)\n",
    "    features[\"bb_lower\"] = (close.shift(1) - (bb_mean - 2 * bb_std)) / close.shift(1)\n",
    "\n",
    "    # Volume features\n",
    "    if \"Volume\" in df.columns:\n",
    "        features[\"volume_ratio\"]  = df[\"Volume\"] / df[\"Volume\"].rolling(20).mean()\n",
    "        features[\"volume_change\"] = df[\"Volume\"].pct_change()\n",
    "\n",
    "    # Calendar features\n",
    "    features[\"day_of_week\"] = df.index.dayofweek\n",
    "    features[\"month\"]       = df.index.month\n",
    "    features[\"quarter\"]     = df.index.quarter\n",
    "\n",
    "    # Lagged returns\n",
    "    for lag in [1, 2, 3, 5, 10]:\n",
    "        features[f\"ret_lag_{lag}\"] = features[\"ret_1d\"].shift(lag)\n",
    "\n",
    "    # Targets\n",
    "    if use_log_return:\n",
    "        next_ret = np.log(close.shift(-horizon) / close)\n",
    "    else:\n",
    "        next_ret = close.pct_change(horizon).shift(-horizon)\n",
    "\n",
    "    target_up = (next_ret > 0).astype(int)\n",
    "\n",
    "    # Clean\n",
    "    features = features.replace([np.inf, -np.inf], np.nan)\n",
    "    data = features.copy()\n",
    "    data[\"next_return\"] = next_ret\n",
    "    data[\"target_up\"]   = target_up\n",
    "\n",
    "    data = data.dropna().copy()\n",
    "    feature_cols = [c for c in data.columns if c not in [\"next_return\", \"target_up\"]]\n",
    "    return data, feature_cols\n",
    "\n",
    "feat_df, feature_cols = build_features_and_labels(df_raw, horizon=1, use_log_return=True)\n",
    "print(f\"Samples after cleaning: {len(feat_df)}, Features: {len(feature_cols)}\")\n",
    "display(feat_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42002e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 3) Turn features into fixed-length sequence windows (lookback)\n",
    "# - X: [num_samples, lookback, num_features]\n",
    "# - y: aligned to the end date of each window (sequence label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def make_sequences(feat_df: pd.DataFrame,\n",
    "                   feature_cols: List[str],\n",
    "                   lookback: int = 60) -> Tuple[np.ndarray, np.ndarray, np.ndarray, pd.DatetimeIndex]:\n",
    "    X_2d = feat_df[feature_cols].astype(float).values\n",
    "    y_cls_all = feat_df[\"target_up\"].astype(int).values\n",
    "    y_reg_all = feat_df[\"next_return\"].astype(float).values\n",
    "    dates = feat_df.index\n",
    "\n",
    "    N = len(feat_df)\n",
    "    F = len(feature_cols)\n",
    "    if N < lookback:\n",
    "        raise ValueError(f\"Too few samples: N={N} < lookback={lookback}\")\n",
    "\n",
    "    X_list, y_cls_list, y_reg_list, idx_list = [], [], [], []\n",
    "    for end in range(lookback - 1, N):\n",
    "        start = end - lookback + 1\n",
    "        X_list.append(X_2d[start:end+1])\n",
    "        y_cls_list.append(y_cls_all[end])\n",
    "        y_reg_list.append(y_reg_all[end])\n",
    "        idx_list.append(dates[end])\n",
    "\n",
    "    X_seq = np.asarray(X_list, dtype=np.float32)\n",
    "    y_cls = np.asarray(y_cls_list, dtype=np.int64)\n",
    "    y_reg = np.asarray(y_reg_list, dtype=np.float32)\n",
    "    seq_index = pd.DatetimeIndex(idx_list)\n",
    "    return X_seq, y_cls, y_reg, seq_index\n",
    "\n",
    "LOOKBACK = 14\n",
    "X_seq_all, y_cls_all, y_reg_all, seq_index = make_sequences(feat_df, feature_cols, lookback=LOOKBACK)\n",
    "print(\"X_seq_all:\", X_seq_all.shape, \"y_cls_all:\", y_cls_all.shape, \"y_reg_all:\", y_reg_all.shape)\n",
    "print(\"First/Last seq date:\", seq_index[0].date(), \"->\", seq_index[-1].date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e039e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 4) Per-feature standardization for 3D sequences (fit on train only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61170f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def fit_scaler_3d(X_train: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    X_train: (N_train, T, F)\n",
    "    Returns per-feature mean/std (shape (F,))\n",
    "    \"\"\"\n",
    "    assert X_train.ndim == 3\n",
    "    N, T, F = X_train.shape\n",
    "    flat = X_train.reshape(-1, F)\n",
    "    mean = flat.mean(axis=0)\n",
    "    std = flat.std(axis=0)\n",
    "    std = np.where(std < 1e-12, 1e-12, std)\n",
    "    return mean.astype(np.float32), std.astype(np.float32)\n",
    "\n",
    "def transform_3d(X: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
    "    return (X - mean[None, None, :]) / std[None, None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db866cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 5) LSTM models (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc9633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=32, num_layers=1, bidirectional=False, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.act = nn.ReLU()\n",
    "        self.out = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):                       # x: [B,T,F]\n",
    "        out, _ = self.lstm(x)                   # [B,T,H]\n",
    "        h = out[:, -1, :]\n",
    "        h = self.dropout(h)\n",
    "        h = self.act(self.fc1(h))\n",
    "        logit = self.out(h)\n",
    "        return logit\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size=32, num_layers=1, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.act = nn.ReLU()\n",
    "        self.out = nn.Linear(hidden_size, 1)  # continuous output\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        h = out[:, -1, :]\n",
    "        h = self.dropout(h)\n",
    "        h = self.act(self.fc1(h))\n",
    "        y = self.out(h)\n",
    "        return y\n",
    "\n",
    "def count_params(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class TrainConfig:\n",
    "    def __init__(self, epochs=200, batch_size=32, patience=20,\n",
    "                 lr=1e-3, weight_decay=1e-4, lr_factor=0.5, lr_patience=10, lr_min=1e-5,\n",
    "                 grad_clip=1.0, verbose=False):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.patience = patience\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.lr_factor = lr_factor\n",
    "        self.lr_patience = lr_patience\n",
    "        self.lr_min = lr_min\n",
    "        self.grad_clip = grad_clip\n",
    "        self.verbose = verbose\n",
    "\n",
    "CFG = TrainConfig(epochs=200, batch_size=32, patience=20,\n",
    "                  lr=1e-3, weight_decay=1e-4, lr_factor=0.5, lr_patience=10, lr_min=1e-5,\n",
    "                  grad_clip=1.0, verbose=False)\n",
    "\n",
    "def make_loader(X: np.ndarray, y: np.ndarray, batch_size: int, shuffle: bool) -> DataLoader:\n",
    "    # Convert y to (N, 1) float32 for both classification and regression\n",
    "    if y.ndim == 1:\n",
    "        y = y[:, None]\n",
    "    X_t = torch.from_numpy(X.astype(np.float32))\n",
    "    y_t = torch.from_numpy(y.astype(np.float32))\n",
    "    ds = TensorDataset(X_t, y_t)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle,\n",
    "                      num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "def train_torch_model(model: nn.Module,\n",
    "                      train_loader: DataLoader,\n",
    "                      val_loader: DataLoader,\n",
    "                      criterion: nn.Module,\n",
    "                      cfg: TrainConfig) -> Tuple[nn.Module, Dict[str, list]]:\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    # Compatible ReduceLROnPlateau across versions\n",
    "    try:\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode='min',\n",
    "            factor=cfg.lr_factor, patience=cfg.lr_patience,\n",
    "            min_lr=cfg.lr_min\n",
    "        )\n",
    "    except TypeError:\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode='min',\n",
    "            factor=cfg.lr_factor, patience=cfg.lr_patience\n",
    "        )\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf')\n",
    "    no_improve = 0\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        # ---- Train ----\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            if cfg.grad_clip is not None:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # ---- Val ----\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device, non_blocking=True)\n",
    "                yb = yb.to(device, non_blocking=True)\n",
    "                pred = model(xb)\n",
    "                loss = criterion(pred, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        if cfg.verbose and (epoch % 10 == 0 or epoch == 1):\n",
    "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            print(f\"Epoch {epoch:03d} | train {train_loss:.6f} | val {val_loss:.6f} | lr {cur_lr:.2e}\")\n",
    "\n",
    "        # ---- Early stopping ----\n",
    "        if val_loss + 1e-12 < best_val:\n",
    "            best_val = val_loss\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= cfg.patience:\n",
    "                if cfg.verbose:\n",
    "                    print(f\"Early stop at epoch {epoch}, best_val={best_val:.6f}\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model, history\n",
    "\n",
    "def predict_torch(model: nn.Module, X: np.ndarray, batch_size: int = 256) -> np.ndarray:\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            xb = torch.from_numpy(X[i:i+batch_size].astype(np.float32)).to(device)\n",
    "            out = model(xb)\n",
    "            preds.append(out.detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds, axis=0)  # (N, 1)\n",
    "    return preds.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74abefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 6) One-fold train wrappers and evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2872cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def train_lstm_cls_reg(X_train, y_cls_train, y_reg_train,\n",
    "                       X_val, y_cls_val, y_reg_val,\n",
    "                       cfg: TrainConfig):\n",
    "    input_size = X_train.shape[-1]\n",
    "    clf = LSTMClassifier(input_size=input_size, hidden_size=32, dropout=0.2)\n",
    "    reg = LSTMRegressor(input_size=input_size, hidden_size=32, dropout=0.2)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader_cls = make_loader(X_train, y_cls_train.astype(np.float32), cfg.batch_size, shuffle=True)\n",
    "    val_loader_cls   = make_loader(X_val,   y_cls_val.astype(np.float32),   cfg.batch_size, shuffle=False)\n",
    "    train_loader_reg = make_loader(X_train, y_reg_train.astype(np.float32), cfg.batch_size, shuffle=True)\n",
    "    val_loader_reg   = make_loader(X_val,   y_reg_val.astype(np.float32),   cfg.batch_size, shuffle=False)\n",
    "\n",
    "    # Losses\n",
    "    crit_cls = nn.BCEWithLogitsLoss()\n",
    "    crit_reg = nn.SmoothL1Loss()  # Huber\n",
    "\n",
    "    # Train\n",
    "    clf, _ = train_torch_model(clf, train_loader_cls, val_loader_cls, crit_cls, cfg)\n",
    "    reg, _ = train_torch_model(reg, train_loader_reg, val_loader_reg, crit_reg, cfg)\n",
    "\n",
    "    return clf, reg\n",
    "\n",
    "def eval_fold_torch(clf, reg, X_test, y_cls_test, y_reg_test) -> Dict[str, float]:\n",
    "    # Classification probability = sigmoid(logit)\n",
    "    logits = predict_torch(clf, X_test, batch_size=256)\n",
    "    proba = 1.0 / (1.0 + np.exp(-logits))\n",
    "    y_pred_cls = (proba >= 0.5).astype(int)\n",
    "\n",
    "    acc  = accuracy_score(y_cls_test, y_pred_cls)\n",
    "    try:\n",
    "        bacc = balanced_accuracy_score(y_cls_test, y_pred_cls)\n",
    "    except Exception:\n",
    "        bacc = np.nan\n",
    "    try:\n",
    "        auc = roc_auc_score(y_cls_test, proba)\n",
    "    except Exception:\n",
    "        auc = np.nan\n",
    "\n",
    "    # Regression\n",
    "    y_pred_reg = predict_torch(reg, X_test, batch_size=256)\n",
    "    reg_mae = mean_absolute_error(y_reg_test, y_pred_reg)\n",
    "    reg_rmse = math.sqrt(mean_squared_error(y_reg_test, y_pred_reg))\n",
    "    try:\n",
    "        reg_r2 = r2_score(y_reg_test, y_pred_reg)\n",
    "    except Exception:\n",
    "        reg_r2 = np.nan\n",
    "    dir_acc = (np.sign(y_reg_test) == np.sign(y_pred_reg)).mean()\n",
    "\n",
    "    return {\n",
    "        \"cls_acc\": acc, \"cls_bacc\": bacc, \"cls_auc\": auc,\n",
    "        \"reg_mae\": reg_mae, \"reg_rmse\": reg_rmse, \"reg_r2\": reg_r2,\n",
    "        \"reg_dir_acc\": dir_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f36bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 7) Fixed-window CV (LSTM, PyTorch)\n",
    "# - Fixed-length train window (`train_window`), followed by `val_size`, then `test_size`.\n",
    "# - Standardize using only the training split in each fold.\n",
    "# - Step forward by `step_size` (default = `test_size`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ab4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def fixed_window_cv_lstm_torch(X_seq_all: np.ndarray,\n",
    "                               y_cls_all: np.ndarray,\n",
    "                               y_reg_all: np.ndarray,\n",
    "                               seq_index: pd.DatetimeIndex,\n",
    "                               test_size: int = 14,\n",
    "                               train_window: int = 400,\n",
    "                               val_size: int = 20,\n",
    "                               step_size: int = None,\n",
    "                               cfg: TrainConfig = CFG) -> pd.DataFrame:\n",
    "    if step_size is None:\n",
    "        step_size = test_size\n",
    "\n",
    "    metrics = []\n",
    "    N = len(X_seq_all)\n",
    "    # First possible test_start index after having train_window + val_size available\n",
    "    start_index = train_window + val_size\n",
    "    fold_id = 0\n",
    "\n",
    "    for test_start in range(start_index, N - test_size + 1, step_size):\n",
    "        train_end = test_start\n",
    "        val_start = train_end - val_size\n",
    "        train_start = val_start - train_window\n",
    "        test_end = test_start + test_size\n",
    "\n",
    "        if train_start < 0:\n",
    "            continue\n",
    "\n",
    "        X_train = X_seq_all[train_start: val_start]\n",
    "        y_cls_train = y_cls_all[train_start: val_start]\n",
    "        y_reg_train = y_reg_all[train_start: val_start]\n",
    "\n",
    "        X_val = X_seq_all[val_start: train_end]\n",
    "        y_cls_val = y_cls_all[val_start: train_end]\n",
    "        y_reg_val = y_reg_all[val_start: train_end]\n",
    "\n",
    "        X_test = X_seq_all[test_start: test_end]\n",
    "        y_cls_test = y_cls_all[test_start: test_end]\n",
    "        y_reg_test = y_reg_all[test_start: test_end]\n",
    "\n",
    "        # Standardize using training only\n",
    "        mean, std = fit_scaler_3d(X_train)\n",
    "        X_train_sc = transform_3d(X_train, mean, std)\n",
    "        X_val_sc   = transform_3d(X_val,   mean, std)\n",
    "        X_test_sc  = transform_3d(X_test,  mean, std)\n",
    "\n",
    "        # Train\n",
    "        clf, reg = train_lstm_cls_reg(X_train_sc, y_cls_train, y_reg_train,\n",
    "                                      X_val_sc,   y_cls_val,   y_reg_val,\n",
    "                                      cfg)\n",
    "\n",
    "        # Evaluate\n",
    "        fold_metrics = eval_fold_torch(clf, reg, X_test_sc, y_cls_test, y_reg_test)\n",
    "        fold_metrics[\"fold\"] = fold_id\n",
    "        fold_metrics[\"train_size\"] = len(X_train_sc)\n",
    "        fold_metrics[\"val_size\"] = len(X_val_sc)\n",
    "        fold_metrics[\"test_size\"] = len(X_test_sc)\n",
    "        fold_metrics[\"test_start\"] = seq_index[test_start]\n",
    "        fold_metrics[\"test_end\"] = seq_index[test_end - 1]\n",
    "        metrics.append(fold_metrics)\n",
    "\n",
    "        fold_id += 1\n",
    "\n",
    "        # Free memory\n",
    "        del clf, reg\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2bcd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 8) Run fixed-window CV and summarize metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c2f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "cv_results = fixed_window_cv_lstm_torch(\n",
    "    X_seq_all, y_cls_all, y_reg_all, seq_index,\n",
    "    test_size=7, train_window=150, val_size=10, step_size=7,\n",
    "    cfg=CFG\n",
    ")\n",
    "\n",
    "print(f\"Folds: {len(cv_results)}\")\n",
    "display(cv_results[[\"fold\",\"test_start\",\"test_end\",\"train_size\",\"val_size\",\"test_size\"]].head())\n",
    "\n",
    "def summarize_metrics(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    return df[cols].agg(['mean','std','min','max'])\n",
    "\n",
    "print(\"\\n=== Per-fold metrics (head) ===\")\n",
    "display(cv_results.head())\n",
    "\n",
    "print(\"\\n=== Summary (Classification) ===\")\n",
    "display(summarize_metrics(cv_results, [\"cls_acc\", \"cls_bacc\", \"cls_auc\"]))\n",
    "\n",
    "print(\"\\n=== Summary (Regression on next_return) ===\")\n",
    "display(summarize_metrics(cv_results, [\"reg_mae\", \"reg_rmse\", \"reg_r2\", \"reg_dir_acc\"]))\n",
    "\n",
    "# Naive baselines over the sequence sample\n",
    "naive_mae = np.abs(y_reg_all).mean()\n",
    "naive_dir_acc = max((y_reg_all > 0).mean(), (y_reg_all <= 0).mean())\n",
    "print(\"\\n=== Naive baselines over sequence sample ===\")\n",
    "print(f\"Regression MAE baseline (predict 0): {naive_mae:.6f}\")\n",
    "print(f\"Direction baseline (majority sign): {naive_dir_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb8c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 9) Visualization: pick a fold (last/best_auc/best_dir) and plot\n",
    "# - Panel 1: next_return actual vs predicted (test window)\n",
    "# - Panel 2: reconstructed price paths (from the previous close), assuming log returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ccb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def pick_fold_index(cv_df, mode='last'):\n",
    "    if len(cv_df) == 0:\n",
    "        raise ValueError(\"cv_results is empty, run CV first.\")\n",
    "    if mode == 'last':\n",
    "        return int(cv_df.iloc[-1]['fold'])\n",
    "    elif mode == 'best_auc':\n",
    "        idx = cv_df['cls_auc'].idxmax()\n",
    "        return int(cv_df.loc[idx, 'fold'])\n",
    "    elif mode == 'best_dir':\n",
    "        idx = cv_df['reg_dir_acc'].idxmax()\n",
    "        return int(cv_df.loc[idx, 'fold'])\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'last' / 'best_auc' / 'best_dir'\")\n",
    "\n",
    "def reconstruct_price_path(start_price: float, returns: np.ndarray, assume_log_return=True) -> np.ndarray:\n",
    "    if assume_log_return:\n",
    "        path = start_price * np.exp(np.cumsum(returns))\n",
    "    else:\n",
    "        path = start_price * np.cumprod(1.0 + returns)\n",
    "    return path\n",
    "\n",
    "def plot_fold_predictions_lstm_fixed(feat_df, feature_cols, df_raw,\n",
    "                                     X_seq_all, y_cls_all, y_reg_all, seq_index,\n",
    "                                     cv_results: pd.DataFrame,\n",
    "                                     fold_to_plot='last', assume_log_return=True,\n",
    "                                     cfg: TrainConfig = CFG):\n",
    "    # Choose fold\n",
    "    fold_idx = pick_fold_index(cv_results, mode=fold_to_plot)\n",
    "    row = cv_results[cv_results['fold'] == fold_idx].iloc[0]\n",
    "    test_start_date = row['test_start']\n",
    "    test_end_date = row['test_end']\n",
    "\n",
    "    # Map dates to positional indices on seq_index\n",
    "    all_dates = seq_index\n",
    "    test_start = all_dates.get_loc(test_start_date)\n",
    "    test_end = all_dates.get_loc(test_end_date) + 1  # right-open\n",
    "\n",
    "    # Reconstruct train/val positions from sizes\n",
    "    train_size = int(row['train_size'])\n",
    "    val_size = int(row['val_size'])\n",
    "    train_end = test_start\n",
    "    val_start = train_end - val_size\n",
    "    train_start = val_start - train_size\n",
    "\n",
    "    if train_start < 0:\n",
    "        raise ValueError(\"Not enough history to reconstruct this fold windows.\")\n",
    "\n",
    "    # Slice arrays\n",
    "    X_train = X_seq_all[train_start: val_start]\n",
    "    y_cls_train = y_cls_all[train_start: val_start]\n",
    "    y_reg_train = y_reg_all[train_start: val_start]\n",
    "\n",
    "    X_val = X_seq_all[val_start: train_end]\n",
    "    y_cls_val = y_cls_all[val_start: train_end]\n",
    "    y_reg_val = y_reg_all[val_start: train_end]\n",
    "\n",
    "    X_test = X_seq_all[test_start: test_end]\n",
    "    y_cls_test = y_cls_all[test_start: test_end]\n",
    "    y_reg_test = y_reg_all[test_start: test_end]\n",
    "    test_dates = all_dates[test_start: test_end]\n",
    "\n",
    "    # Standardize using training only\n",
    "    mean, std = fit_scaler_3d(X_train)\n",
    "    X_train_sc = transform_3d(X_train, mean, std)\n",
    "    X_val_sc   = transform_3d(X_val,   mean, std)\n",
    "    X_test_sc  = transform_3d(X_test,  mean, std)\n",
    "\n",
    "    # Train\n",
    "    clf, reg = train_lstm_cls_reg(X_train_sc, y_cls_train, y_reg_train,\n",
    "                                  X_val_sc,   y_cls_val,   y_reg_val,\n",
    "                                  cfg)\n",
    "\n",
    "    # Predict\n",
    "    logits = predict_torch(clf, X_test_sc, batch_size=256)\n",
    "    proba = 1.0 / (1.0 + np.exp(-logits))\n",
    "    y_pred_cls = (proba >= 0.5).astype(int)\n",
    "    y_pred_reg = predict_torch(reg, X_test_sc, batch_size=256)\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "    ax1, ax2 = axes\n",
    "\n",
    "    # Panel 1: next_return\n",
    "    ax1.plot(test_dates, y_reg_test, label='Actual next_return', color='tab:blue')\n",
    "    ax1.plot(test_dates, y_pred_reg, label='Predicted next_return', color='tab:orange', alpha=0.8)\n",
    "    ax1.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax1.set_title(f'Fold {fold_idx}: next_return - Actual vs Predicted (LSTM - PyTorch)')\n",
    "    ax1.set_ylabel('Return')\n",
    "    ax1.legend(loc='best')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Panel 2: price path reconstruction (from previous close)\n",
    "    feat_index = feat_df.index\n",
    "    close_aligned = df_raw.loc[feat_index, 'Close']\n",
    "\n",
    "    pos_in_feat = feat_index.get_loc(test_dates[0])\n",
    "    if pos_in_feat - 1 >= 0:\n",
    "        start_close = float(close_aligned.iloc[pos_in_feat - 1])\n",
    "        actual_price_path = reconstruct_price_path(start_close, y_reg_test, assume_log_return)\n",
    "        pred_price_path   = reconstruct_price_path(start_close, y_pred_reg, assume_log_return)\n",
    "        price_index = [feat_index[pos_in_feat - 1]] + list(test_dates)\n",
    "        ax2.plot(price_index, [start_close] + list(actual_price_path), label='Actual price (reconstructed)', color='tab:green')\n",
    "        ax2.plot(price_index, [start_close] + list(pred_price_path), label='Predicted price (reconstructed)', color='tab:red', alpha=0.8)\n",
    "        ax2.set_title(f'Fold {fold_idx}: Price path (from previous close)')\n",
    "        ax2.set_ylabel('Price')\n",
    "        ax2.legend(loc='best')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax2.set_visible(False)\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Metrics\n",
    "    acc  = accuracy_score(y_cls_test, y_pred_cls)\n",
    "    bacc = balanced_accuracy_score(y_cls_test, y_pred_cls)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_cls_test, proba)\n",
    "    except Exception:\n",
    "        auc = np.nan\n",
    "    dir_acc = (np.sign(y_reg_test) == np.sign(y_pred_reg)).mean()\n",
    "    print(f\"Fold {fold_idx} metrics:\")\n",
    "    print(f\"  Classification - Acc: {acc:.3f}, BalancedAcc: {bacc:.3f}, AUC: {auc:.3f}\")\n",
    "    print(f\"  Regression - MAE: {mean_absolute_error(y_reg_test, y_pred_reg):.6f}, \"\n",
    "          f\"R2: {r2_score(y_reg_test, y_pred_reg):.6f}, \"\n",
    "          f\"DirAcc: {dir_acc:.3f}\")\n",
    "\n",
    "# Plot the last fold by default\n",
    "plot_fold_predictions_lstm_fixed(\n",
    "    feat_df=feat_df,\n",
    "    feature_cols=feature_cols,\n",
    "    df_raw=df_raw,\n",
    "    X_seq_all=X_seq_all, y_cls_all=y_cls_all, y_reg_all=y_reg_all, seq_index=seq_index,\n",
    "    cv_results=cv_results,\n",
    "    fold_to_plot='last',   # 'last' / 'best_auc' / 'best_dir'\n",
    "    assume_log_return=True,\n",
    "    cfg=CFG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ac2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 10) OOF (out-of-fold) classification for fixed-window CV\n",
    "# - Generate OOF probabilities across all folds using fixed windows.\n",
    "# - Each fold re-standardizes on its training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebbefbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def oof_fixed_window_lstm_torch(X_seq_all, y_cls_all, seq_index,\n",
    "                                test_size=7, train_window=252, val_size=20, step_size=None,\n",
    "                                cfg: TrainConfig = CFG):\n",
    "    if step_size is None:\n",
    "        step_size = test_size\n",
    "\n",
    "    N = len(X_seq_all)\n",
    "    start_index = train_window + val_size\n",
    "\n",
    "    oof_true, oof_proba, oof_pred = [], [], []\n",
    "    fold_id = 0\n",
    "\n",
    "    for test_start in range(start_index, N - test_size + 1, step_size):\n",
    "        train_end = test_start\n",
    "        val_start = train_end - val_size\n",
    "        train_start = val_start - train_window\n",
    "        test_end = test_start + test_size\n",
    "        if train_start < 0:\n",
    "            continue\n",
    "\n",
    "        X_train = X_seq_all[train_start: val_start]\n",
    "        y_train_cls = y_cls_all[train_start: val_start]\n",
    "        X_val = X_seq_all[val_start: train_end]\n",
    "        y_val_cls = y_cls_all[val_start: train_end]\n",
    "        X_test = X_seq_all[test_start: test_end]\n",
    "        y_test_cls = y_cls_all[test_start: test_end]\n",
    "\n",
    "        # Standardize\n",
    "        mean, std = fit_scaler_3d(X_train)\n",
    "        X_train_sc = transform_3d(X_train, mean, std)\n",
    "        X_val_sc   = transform_3d(X_val,   mean, std)\n",
    "        X_test_sc  = transform_3d(X_test,  mean, std)\n",
    "\n",
    "        # Train classifier only\n",
    "        input_size = X_train_sc.shape[-1]\n",
    "        clf = LSTMClassifier(input_size=input_size, hidden_size=32, dropout=0.2)\n",
    "        crit_cls = nn.BCEWithLogitsLoss()\n",
    "        train_loader_cls = make_loader(X_train_sc, y_train_cls.astype(np.float32), cfg.batch_size, shuffle=True)\n",
    "        val_loader_cls   = make_loader(X_val_sc,   y_val_cls.astype(np.float32),   cfg.batch_size, shuffle=False)\n",
    "        clf, _ = train_torch_model(clf, train_loader_cls, val_loader_cls, crit_cls, cfg)\n",
    "\n",
    "        logits = predict_torch(clf, X_test_sc, batch_size=256)\n",
    "        proba = 1.0 / (1.0 + np.exp(-logits))\n",
    "        pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "        oof_true.append(y_test_cls)\n",
    "        oof_proba.append(proba)\n",
    "        oof_pred.append(pred)\n",
    "\n",
    "        fold_id += 1\n",
    "        del clf\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    y_true = np.concatenate(oof_true) if len(oof_true) else np.array([])\n",
    "    y_proba = np.concatenate(oof_proba) if len(oof_proba) else np.array([])\n",
    "    y_pred = np.concatenate(oof_pred) if len(oof_pred) else np.array([])\n",
    "    return y_true, y_proba, y_pred\n",
    "\n",
    "y_true, y_proba, y_pred = oof_fixed_window_lstm_torch(\n",
    "    X_seq_all, y_cls_all, seq_index,\n",
    "    test_size=7, train_window=252, val_size=20, step_size=7,\n",
    "    cfg=CFG\n",
    ")\n",
    "\n",
    "if len(y_true):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_proba)\n",
    "    except Exception:\n",
    "        auc = np.nan\n",
    "\n",
    "    print(f\"OOF Accuracy={acc:.3f}, BalancedAcc={bacc:.3f}, AUC={auc:.3f}, N={len(y_true)}\")\n",
    "\n",
    "    # Approx 95% CI (Wilson-like normal approx)\n",
    "    from math import sqrt\n",
    "    n = len(y_true)\n",
    "    phat = acc\n",
    "    z = 1.96\n",
    "    half = z*sqrt(phat*(1-phat)/n)\n",
    "    print(f\"Acc 95% CI (approx): [{phat - half:.3f}, {phat + half:.3f}]\")\n",
    "else:\n",
    "    print(\"OOF not generated (insufficient samples or parameter settings).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "65afdfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "数据范围: 2024-10-07 ~ 2025-10-06, 共 246 根K线\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-09-26</th>\n",
       "      <td>26.980000</td>\n",
       "      <td>27.059999</td>\n",
       "      <td>26.719999</td>\n",
       "      <td>26.780001</td>\n",
       "      <td>565463647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-29</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.340000</td>\n",
       "      <td>26.980000</td>\n",
       "      <td>27.240000</td>\n",
       "      <td>698376064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-30</th>\n",
       "      <td>27.280001</td>\n",
       "      <td>27.559999</td>\n",
       "      <td>27.200001</td>\n",
       "      <td>27.520000</td>\n",
       "      <td>383166715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-03</th>\n",
       "      <td>27.879999</td>\n",
       "      <td>27.920000</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>27.780001</td>\n",
       "      <td>164529440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-06</th>\n",
       "      <td>27.700001</td>\n",
       "      <td>27.799999</td>\n",
       "      <td>27.520000</td>\n",
       "      <td>27.620001</td>\n",
       "      <td>230855946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close     Volume\n",
       "Date                                                             \n",
       "2025-09-26  26.980000  27.059999  26.719999  26.780001  565463647\n",
       "2025-09-29  27.000000  27.340000  26.980000  27.240000  698376064\n",
       "2025-09-30  27.280001  27.559999  27.200001  27.520000  383166715\n",
       "2025-10-03  27.879999  27.920000  27.600000  27.780001  164529440\n",
       "2025-10-06  27.700001  27.799999  27.520000  27.620001  230855946"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清洗后样本数: 222, 特征数: 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ret_1d</th>\n",
       "      <th>ret_2d</th>\n",
       "      <th>ret_5d</th>\n",
       "      <th>ret_10d</th>\n",
       "      <th>vol_5d</th>\n",
       "      <th>vol_10d</th>\n",
       "      <th>vol_20d</th>\n",
       "      <th>close_to_sma5</th>\n",
       "      <th>close_to_sma10</th>\n",
       "      <th>close_to_sma20</th>\n",
       "      <th>...</th>\n",
       "      <th>volume_change</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>quarter</th>\n",
       "      <th>ret_lag_1</th>\n",
       "      <th>ret_lag_2</th>\n",
       "      <th>ret_lag_3</th>\n",
       "      <th>ret_lag_5</th>\n",
       "      <th>ret_lag_10</th>\n",
       "      <th>next_return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-09-25</th>\n",
       "      <td>-0.001472</td>\n",
       "      <td>0.012687</td>\n",
       "      <td>-0.001472</td>\n",
       "      <td>0.015719</td>\n",
       "      <td>0.008895</td>\n",
       "      <td>0.009729</td>\n",
       "      <td>0.009874</td>\n",
       "      <td>0.004583</td>\n",
       "      <td>0.002508</td>\n",
       "      <td>0.020462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.564963</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.014179</td>\n",
       "      <td>-0.005935</td>\n",
       "      <td>-0.008824</td>\n",
       "      <td>-0.012355</td>\n",
       "      <td>-0.002985</td>\n",
       "      <td>-0.013353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-26</th>\n",
       "      <td>-0.013265</td>\n",
       "      <td>-0.014717</td>\n",
       "      <td>-0.015441</td>\n",
       "      <td>-0.008148</td>\n",
       "      <td>0.010551</td>\n",
       "      <td>0.010208</td>\n",
       "      <td>0.010523</td>\n",
       "      <td>0.006229</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>0.016746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016764</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.001472</td>\n",
       "      <td>0.014179</td>\n",
       "      <td>-0.005935</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.010479</td>\n",
       "      <td>0.017031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-29</th>\n",
       "      <td>0.017177</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>0.010386</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>0.013101</td>\n",
       "      <td>0.011670</td>\n",
       "      <td>0.010187</td>\n",
       "      <td>-0.009176</td>\n",
       "      <td>-0.012027</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235050</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.013265</td>\n",
       "      <td>-0.001472</td>\n",
       "      <td>0.014179</td>\n",
       "      <td>-0.008824</td>\n",
       "      <td>0.002963</td>\n",
       "      <td>0.010227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-30</th>\n",
       "      <td>0.010279</td>\n",
       "      <td>0.027633</td>\n",
       "      <td>0.026866</td>\n",
       "      <td>0.016999</td>\n",
       "      <td>0.012601</td>\n",
       "      <td>0.012039</td>\n",
       "      <td>0.010218</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>0.015622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.451346</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.017177</td>\n",
       "      <td>-0.013265</td>\n",
       "      <td>-0.001472</td>\n",
       "      <td>-0.005935</td>\n",
       "      <td>-0.000739</td>\n",
       "      <td>0.009403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-03</th>\n",
       "      <td>0.009448</td>\n",
       "      <td>0.019824</td>\n",
       "      <td>0.022075</td>\n",
       "      <td>0.009448</td>\n",
       "      <td>0.011935</td>\n",
       "      <td>0.011183</td>\n",
       "      <td>0.010130</td>\n",
       "      <td>0.008354</td>\n",
       "      <td>0.012584</td>\n",
       "      <td>0.022478</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.570606</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010279</td>\n",
       "      <td>0.017177</td>\n",
       "      <td>-0.013265</td>\n",
       "      <td>0.014179</td>\n",
       "      <td>0.016999</td>\n",
       "      <td>-0.005776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ret_1d    ret_2d    ret_5d   ret_10d    vol_5d   vol_10d  \\\n",
       "Date                                                                     \n",
       "2025-09-25 -0.001472  0.012687 -0.001472  0.015719  0.008895  0.009729   \n",
       "2025-09-26 -0.013265 -0.014717 -0.015441 -0.008148  0.010551  0.010208   \n",
       "2025-09-29  0.017177  0.003685  0.010386  0.005908  0.013101  0.011670   \n",
       "2025-09-30  0.010279  0.027633  0.026866  0.016999  0.012601  0.012039   \n",
       "2025-10-03  0.009448  0.019824  0.022075  0.009448  0.011935  0.011183   \n",
       "\n",
       "             vol_20d  close_to_sma5  close_to_sma10  close_to_sma20  ...  \\\n",
       "Date                                                                 ...   \n",
       "2025-09-25  0.009874       0.004583        0.002508        0.020462  ...   \n",
       "2025-09-26  0.010523       0.006229        0.001846        0.016746  ...   \n",
       "2025-09-29  0.010187      -0.009176       -0.012027        0.001234  ...   \n",
       "2025-09-30  0.010218       0.002503        0.003241        0.015622  ...   \n",
       "2025-10-03  0.010130       0.008354        0.012584        0.022478  ...   \n",
       "\n",
       "            volume_change  day_of_week  month  quarter  ret_lag_1  ret_lag_2  \\\n",
       "Date                                                                           \n",
       "2025-09-25       0.564963            3      9        3   0.014179  -0.005935   \n",
       "2025-09-26       0.016764            4      9        3  -0.001472   0.014179   \n",
       "2025-09-29       0.235050            0      9        3  -0.013265  -0.001472   \n",
       "2025-09-30      -0.451346            1      9        3   0.017177  -0.013265   \n",
       "2025-10-03      -0.570606            4     10        4   0.010279   0.017177   \n",
       "\n",
       "            ret_lag_3  ret_lag_5  ret_lag_10  next_return  \n",
       "Date                                                       \n",
       "2025-09-25  -0.008824  -0.012355   -0.002985    -0.013353  \n",
       "2025-09-26  -0.005935   0.000736    0.010479     0.017031  \n",
       "2025-09-29   0.014179  -0.008824    0.002963     0.010227  \n",
       "2025-09-30  -0.001472  -0.005935   -0.000739     0.009403  \n",
       "2025-10-03  -0.013265   0.014179    0.016999    -0.005776  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "阈值搜索结果（按准确率排序）:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>mean_acc</th>\n",
       "      <th>std_acc</th>\n",
       "      <th>mean_bacc</th>\n",
       "      <th>mean_macro_f1</th>\n",
       "      <th>folds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.070696</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.064536</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0142</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.049990</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0155</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.049990</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.104062</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.397575</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.107990</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.359862</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.149969</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.321334</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.221689</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.300459</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.202031</td>\n",
       "      <td>0.353175</td>\n",
       "      <td>0.224868</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.170747</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0.274668</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.234472</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.202683</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.147165</td>\n",
       "      <td>0.318254</td>\n",
       "      <td>0.212434</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    threshold  mean_acc   std_acc  mean_bacc  mean_macro_f1  folds\n",
       "0      0.0180  0.938776  0.070696   0.785714       0.769231      7\n",
       "1      0.0167  0.897959  0.064536   0.642857       0.615385      7\n",
       "2      0.0142  0.877551  0.049990   0.571429       0.538462      7\n",
       "3      0.0155  0.877551  0.049990   0.571429       0.538462      7\n",
       "4      0.0129  0.775510  0.104062   0.452381       0.397575      7\n",
       "5      0.0116  0.714286  0.107990   0.428571       0.359862      7\n",
       "6      0.0104  0.653061  0.149969   0.404762       0.321334      7\n",
       "7      0.0091  0.591837  0.221689   0.404762       0.300459      7\n",
       "8      0.0040  0.428571  0.202031   0.353175       0.224868      7\n",
       "9      0.0065  0.428571  0.170747   0.409524       0.274668      7\n",
       "10     0.0078  0.408163  0.234472   0.357143       0.202683      7\n",
       "11     0.0053  0.326531  0.147165   0.318254       0.212434      7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "选定阈值: 1.04%\n",
      "threshold        0.010400\n",
      "mean_acc         0.653061\n",
      "std_acc          0.149969\n",
      "mean_bacc        0.404762\n",
      "mean_macro_f1    0.321334\n",
      "folds            7.000000\n",
      "Name: 6, dtype: float64\n",
      "共生成 7 个折。\n",
      "交叉验证折指标（前 5 行）:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>acc</th>\n",
       "      <th>bacc</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>train_size</th>\n",
       "      <th>val_size</th>\n",
       "      <th>test_size</th>\n",
       "      <th>test_start</th>\n",
       "      <th>test_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2025-07-25</td>\n",
       "      <td>2025-08-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2025-08-05</td>\n",
       "      <td>2025-08-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2025-08-14</td>\n",
       "      <td>2025-08-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2025-08-25</td>\n",
       "      <td>2025-09-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2025-09-03</td>\n",
       "      <td>2025-09-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fold       acc      bacc  macro_f1  train_size  val_size  test_size  \\\n",
       "0     0  0.571429  0.500000  0.363636         150        10          7   \n",
       "1     1  0.857143  0.500000  0.461538         150        10          7   \n",
       "2     2  0.857143  0.500000  0.461538         150        10          7   \n",
       "3     3  0.571429  0.333333  0.242424         150        10          7   \n",
       "4     4  0.428571  0.333333  0.200000         150        10          7   \n",
       "\n",
       "  test_start   test_end  \n",
       "0 2025-07-25 2025-08-04  \n",
       "1 2025-08-05 2025-08-13  \n",
       "2 2025-08-14 2025-08-22  \n",
       "3 2025-08-25 2025-09-02  \n",
       "4 2025-09-03 2025-09-11  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "交叉验证指标汇总:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>bacc</th>\n",
       "      <th>macro_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.321334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.161985</td>\n",
       "      <td>0.089087</td>\n",
       "      <td>0.108149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           acc      bacc  macro_f1\n",
       "mean  0.653061  0.404762  0.321334\n",
       "std   0.161985  0.089087  0.108149\n",
       "min   0.428571  0.333333  0.200000\n",
       "max   0.857143  0.500000  0.461538"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整体准确率: 0.653\n",
      "整体平衡准确率: 0.333\n",
      "Macro-F1: 0.263\n",
      "\n",
      "分类报告:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "显著下跌 (≤ -1.04%)      0.000     0.000     0.000         8\n",
      "          无显著波动      0.653     1.000     0.790        32\n",
      " 显著上涨 (≥ 1.04%)      0.000     0.000     0.000         9\n",
      "\n",
      "       accuracy                          0.653        49\n",
      "      macro avg      0.218     0.333     0.263        49\n",
      "   weighted avg      0.426     0.653     0.516        49\n",
      "\n",
      "混淆矩阵:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>显著下跌 (≤ -1.04%)</th>\n",
       "      <th>无显著波动</th>\n",
       "      <th>显著上涨 (≥ 1.04%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>显著下跌 (≤ -1.04%)</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>无显著波动</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>显著上涨 (≥ 1.04%)</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 显著下跌 (≤ -1.04%)  无显著波动  显著上涨 (≥ 1.04%)\n",
       "显著下跌 (≤ -1.04%)                0      8               0\n",
       "无显著波动                          0     32               0\n",
       "显著上涨 (≥ 1.04%)                 0      9               0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略样本预览:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>date</th>\n",
       "      <th>actual_class</th>\n",
       "      <th>pred_class</th>\n",
       "      <th>actual_return</th>\n",
       "      <th>proba_0</th>\n",
       "      <th>proba_1</th>\n",
       "      <th>proba_2</th>\n",
       "      <th>position</th>\n",
       "      <th>simple_return</th>\n",
       "      <th>strategy_simple_return</th>\n",
       "      <th>transaction_cost</th>\n",
       "      <th>strategy_simple_return_after_cost</th>\n",
       "      <th>strategy_equity</th>\n",
       "      <th>buyhold_equity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006931</td>\n",
       "      <td>0.156966</td>\n",
       "      <td>0.628160</td>\n",
       "      <td>0.214874</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.006955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.001536</td>\n",
       "      <td>0.169876</td>\n",
       "      <td>0.620187</td>\n",
       "      <td>0.209937</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.001535</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.005410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-29</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.013932</td>\n",
       "      <td>0.191363</td>\n",
       "      <td>0.580664</td>\n",
       "      <td>0.227974</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.013836</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.015711</td>\n",
       "      <td>0.179064</td>\n",
       "      <td>0.592177</td>\n",
       "      <td>0.228758</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.015588</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.976043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-07-31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.011147</td>\n",
       "      <td>0.170003</td>\n",
       "      <td>0.598194</td>\n",
       "      <td>0.231803</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.011085</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.965224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fold       date  actual_class  pred_class  actual_return   proba_0  \\\n",
       "0     0 2025-07-25             1           1       0.006931  0.156966   \n",
       "1     0 2025-07-28             1           1      -0.001536  0.169876   \n",
       "2     0 2025-07-29             0           1      -0.013932  0.191363   \n",
       "3     0 2025-07-30             0           1      -0.015711  0.179064   \n",
       "4     0 2025-07-31             0           1      -0.011147  0.170003   \n",
       "\n",
       "    proba_1   proba_2  position  simple_return  strategy_simple_return  \\\n",
       "0  0.628160  0.214874         0       0.006955                     0.0   \n",
       "1  0.620187  0.209937         0      -0.001535                    -0.0   \n",
       "2  0.580664  0.227974         0      -0.013836                    -0.0   \n",
       "3  0.592177  0.228758         0      -0.015588                    -0.0   \n",
       "4  0.598194  0.231803         0      -0.011085                    -0.0   \n",
       "\n",
       "   transaction_cost  strategy_simple_return_after_cost  strategy_equity  \\\n",
       "0               0.0                                0.0              1.0   \n",
       "1               0.0                               -0.0              1.0   \n",
       "2               0.0                               -0.0              1.0   \n",
       "3               0.0                               -0.0              1.0   \n",
       "4               0.0                               -0.0              1.0   \n",
       "\n",
       "   buyhold_equity  \n",
       "0        1.006955  \n",
       "1        1.005410  \n",
       "2        0.991499  \n",
       "3        0.976043  \n",
       "4        0.965224  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "策略表现指标:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>值</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>总收益</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>买入持有总收益</th>\n",
       "      <td>0.067233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>年化收益率</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>买入持有年化收益率</th>\n",
       "      <td>0.397442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>年化波动率</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>夏普率</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>最大回撤</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>交易日数</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>交易准确率</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>单次交易平均收益</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>多头占比</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>空头占比</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>空仓占比</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>交易成本 (bps)</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   值\n",
       "总收益         0.000000\n",
       "买入持有总收益     0.067233\n",
       "年化收益率       0.000000\n",
       "买入持有年化收益率   0.397442\n",
       "年化波动率       0.000000\n",
       "夏普率              NaN\n",
       "最大回撤        0.000000\n",
       "交易日数        0.000000\n",
       "交易准确率            NaN\n",
       "单次交易平均收益         NaN\n",
       "多头占比        0.000000\n",
       "空头占比        0.000000\n",
       "空仓占比        1.000000\n",
       "交易成本 (bps)  2.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Fixed-window LSTM 多类别分类：显著涨跌识别 + 策略回测\n",
    "# \n",
    "# - 目标：用固定窗口交叉验证训练多类别 LSTM，对次日**显著上涨 / 显著下跌 / 无显著波动**进行分类预测。\n",
    "# - 阈值搜索：在候选阈值集合中自动寻找达到 65%~70% 准确率的阈值。\n",
    "# - 交易策略：按预测信号买入“显著上涨”ETF、卖出（做空）“显著下跌”ETF，输出回测表现。\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. 环境与依赖\n",
    "\n",
    "# %%\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os, sys, math, random, copy\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid') if 'seaborn-v0_8-whitegrid' in plt.style.available else plt.style.use('ggplot')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# 设备与随机种子\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. 数据获取\n",
    "\n",
    "# %%\n",
    "def fetch_prices(ticker: str = \"2800.HK\", period: str = \"1y\", interval: str = \"1d\") -> pd.DataFrame:\n",
    "    tk = yf.Ticker(ticker)\n",
    "    df = tk.history(period=period, interval=interval, auto_adjust=True)\n",
    "    if df is None or df.empty:\n",
    "        end = pd.Timestamp.today().normalize()\n",
    "        start = end - pd.Timedelta(days=3 * 370)\n",
    "        df = tk.history(start=start, end=end, interval=interval, auto_adjust=True)\n",
    "    if df is None or df.empty:\n",
    "        raise RuntimeError(f\"无法获取 {ticker} 数据。\")\n",
    "    df.index = pd.to_datetime(df.index).tz_localize(None)\n",
    "    return df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].copy()\n",
    "\n",
    "TICKER = \"2800.HK\"\n",
    "df_raw = fetch_prices(ticker=TICKER, period=\"1y\", interval=\"1d\")\n",
    "print(f\"数据范围: {df_raw.index.min().date()} ~ {df_raw.index.max().date()}, 共 {len(df_raw)} 根K线\")\n",
    "display(df_raw.tail())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. 特征工程（保留上一版指标，仅去除回归标签）\n",
    "\n",
    "# %%\n",
    "def calculate_rsi(series: pd.Series, period: int = 14) -> pd.Series:\n",
    "    delta = series.diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -delta.clip(upper=0)\n",
    "    roll_up = up.ewm(alpha=1 / period, adjust=False).mean()\n",
    "    roll_down = down.ewm(alpha=1 / period, adjust=False).mean().replace(0, np.nan)\n",
    "    rs = roll_up / roll_down\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def calculate_macd(close: pd.Series, fast=12, slow=26, signal=9):\n",
    "    ema_fast = close.ewm(span=fast, adjust=False).mean()\n",
    "    ema_slow = close.ewm(span=slow, adjust=False).mean()\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
    "    histogram = macd_line - signal_line\n",
    "    return macd_line, signal_line, histogram\n",
    "\n",
    "def build_features(df: pd.DataFrame, horizon: int = 1, use_log_return: bool = True):\n",
    "    close = df[\"Close\"].copy()\n",
    "    feat = pd.DataFrame(index=df.index)\n",
    "\n",
    "    feat[\"ret_1d\"] = close.pct_change(1)\n",
    "    feat[\"ret_2d\"] = close.pct_change(2)\n",
    "    feat[\"ret_5d\"] = close.pct_change(5)\n",
    "    feat[\"ret_10d\"] = close.pct_change(10)\n",
    "\n",
    "    feat[\"vol_5d\"] = feat[\"ret_1d\"].rolling(5).std()\n",
    "    feat[\"vol_10d\"] = feat[\"ret_1d\"].rolling(10).std()\n",
    "    feat[\"vol_20d\"] = feat[\"ret_1d\"].rolling(20).std()\n",
    "\n",
    "    sma_5 = close.rolling(5).mean()\n",
    "    sma_10 = close.rolling(10).mean()\n",
    "    sma_20 = close.rolling(20).mean()\n",
    "    sma_50 = close.rolling(50).mean()\n",
    "\n",
    "    feat[\"close_to_sma5\"] = close.shift(1) / sma_5 - 1\n",
    "    feat[\"close_to_sma10\"] = close.shift(1) / sma_10 - 1\n",
    "    feat[\"close_to_sma20\"] = close.shift(1) / sma_20 - 1\n",
    "    feat[\"sma5_sma10\"] = sma_5 / sma_10 - 1\n",
    "    feat[\"sma10_sma20\"] = sma_10 / sma_20 - 1\n",
    "\n",
    "    feat[\"rsi_14\"] = calculate_rsi(close.shift(1), 14)\n",
    "    macd_line, signal_line, histogram = calculate_macd(close.shift(1))\n",
    "    feat[\"macd\"] = macd_line\n",
    "    feat[\"macd_signal\"] = signal_line\n",
    "    feat[\"macd_hist\"] = histogram\n",
    "\n",
    "    bb_period = 20\n",
    "    bb_std = close.rolling(bb_period).std()\n",
    "    bb_mean = close.rolling(bb_period).mean()\n",
    "    feat[\"bb_upper\"] = (bb_mean + 2 * bb_std - close.shift(1)) / close.shift(1)\n",
    "    feat[\"bb_lower\"] = (close.shift(1) - (bb_mean - 2 * bb_std)) / close.shift(1)\n",
    "\n",
    "    if \"Volume\" in df.columns:\n",
    "        feat[\"volume_ratio\"] = df[\"Volume\"] / df[\"Volume\"].rolling(20).mean()\n",
    "        feat[\"volume_change\"] = df[\"Volume\"].pct_change()\n",
    "\n",
    "    feat[\"day_of_week\"] = df.index.dayofweek\n",
    "    feat[\"month\"] = df.index.month\n",
    "    feat[\"quarter\"] = df.index.quarter\n",
    "\n",
    "    for lag in [1, 2, 3, 5, 10]:\n",
    "        feat[f\"ret_lag_{lag}\"] = feat[\"ret_1d\"].shift(lag)\n",
    "\n",
    "    if use_log_return:\n",
    "        next_ret = np.log(close.shift(-horizon) / close)\n",
    "    else:\n",
    "        next_ret = close.pct_change(horizon).shift(-horizon)\n",
    "\n",
    "    feat = feat.replace([np.inf, -np.inf], np.nan)\n",
    "    data = feat.copy()\n",
    "    data[\"next_return\"] = next_ret\n",
    "\n",
    "    data = data.dropna().copy()\n",
    "    feature_cols = [c for c in data.columns if c != \"next_return\"]\n",
    "\n",
    "    return data, feature_cols\n",
    "\n",
    "feat_df, feature_cols = build_features(df_raw, horizon=1, use_log_return=True)\n",
    "print(f\"清洗后样本数: {len(feat_df)}, 特征数: {len(feature_cols)}\")\n",
    "display(feat_df.tail())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. 序列构建（lookback 窗）\n",
    "\n",
    "# %%\n",
    "def make_sequences(\n",
    "    feat_df: pd.DataFrame,\n",
    "    feature_cols: List[str],\n",
    "    lookback: int,\n",
    "    label_cols: List[str]\n",
    ") -> Tuple[np.ndarray, Dict[str, np.ndarray], pd.DatetimeIndex]:\n",
    "    X_2d = feat_df[feature_cols].astype(np.float32).values\n",
    "    dates = feat_df.index\n",
    "    N = len(feat_df)\n",
    "    if N < lookback:\n",
    "        raise ValueError(f\"样本不足: N={N} < lookback={lookback}\")\n",
    "\n",
    "    X_list = []\n",
    "    labels_dict = {col: [] for col in label_cols}\n",
    "    idx_list = []\n",
    "\n",
    "    for end in range(lookback - 1, N):\n",
    "        start = end - lookback + 1\n",
    "        X_list.append(X_2d[start:end + 1])\n",
    "        idx_list.append(dates[end])\n",
    "        for col in label_cols:\n",
    "            labels_dict[col].append(feat_df.iloc[end][col])\n",
    "\n",
    "    X_seq = np.stack(X_list).astype(np.float32)\n",
    "    labels_dict = {col: np.asarray(vals) for col, vals in labels_dict.items()}\n",
    "    seq_index = pd.DatetimeIndex(idx_list)\n",
    "    return X_seq, labels_dict, seq_index\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. 标签定义与阈值工具\n",
    "\n",
    "# %%\n",
    "CLASS_ID_DOWN = 0\n",
    "CLASS_ID_NEUTRAL = 1\n",
    "CLASS_ID_UP = 2\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "def label_significant_moves(next_returns: pd.Series, threshold: float) -> pd.Series:\n",
    "    if threshold <= 0:\n",
    "        raise ValueError(\"threshold 必须为正数。\")\n",
    "    labels = pd.Series(\n",
    "        np.full(len(next_returns), CLASS_ID_NEUTRAL, dtype=np.int32),\n",
    "        index=next_returns.index\n",
    "    )\n",
    "    labels[next_returns >= threshold] = CLASS_ID_UP\n",
    "    labels[next_returns <= -threshold] = CLASS_ID_DOWN\n",
    "    return labels\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. 标准化工具\n",
    "\n",
    "# %%\n",
    "def fit_scaler_3d(X_train: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    assert X_train.ndim == 3\n",
    "    flat = X_train.reshape(-1, X_train.shape[-1])\n",
    "    mean = flat.mean(axis=0)\n",
    "    std = flat.std(axis=0)\n",
    "    std = np.where(std < 1e-12, 1e-12, std)\n",
    "    return mean.astype(np.float32), std.astype(np.float32)\n",
    "\n",
    "def transform_3d(X: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
    "    return (X - mean[None, None, :]) / std[None, None, :]\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. LSTM 模型与训练封装（多类别）\n",
    "\n",
    "# %%\n",
    "class LSTMMultiClassifier(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int = 64, num_layers: int = 1, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.act = nn.ReLU()\n",
    "        self.out = nn.Linear(hidden_size, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        h = out[:, -1, :]\n",
    "        h = self.dropout(h)\n",
    "        h = self.act(self.fc1(h))\n",
    "        logits = self.out(h)\n",
    "        return logits\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    epochs: int = 120\n",
    "    batch_size: int = 64\n",
    "    patience: int = 15\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    lr_factor: float = 0.5\n",
    "    lr_patience: int = 8\n",
    "    lr_min: float = 1e-5\n",
    "    grad_clip: float = 1.0\n",
    "    verbose: bool = False\n",
    "\n",
    "CFG = TrainConfig()\n",
    "\n",
    "def make_loader_classification(X: np.ndarray, y: np.ndarray, batch_size: int, shuffle: bool) -> DataLoader:\n",
    "    X_t = torch.from_numpy(X.astype(np.float32))\n",
    "    y_t = torch.from_numpy(y.astype(np.int64))\n",
    "    ds = TensorDataset(X_t, y_t)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=0, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "def train_classifier(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, cfg: TrainConfig) -> Tuple[nn.Module, Dict[str, List[float]]]:\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    try:\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode='min',\n",
    "            factor=cfg.lr_factor, patience=cfg.lr_patience,\n",
    "            min_lr=cfg.lr_min\n",
    "        )\n",
    "    except TypeError:\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode='min',\n",
    "            factor=cfg.lr_factor, patience=cfg.lr_patience\n",
    "        )\n",
    "\n",
    "    best_state = copy.deepcopy(model.state_dict())\n",
    "    best_val = float('inf')\n",
    "    no_improve = 0\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            if cfg.grad_clip is not None:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device, non_blocking=True)\n",
    "                yb = yb.to(device, non_blocking=True)\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        if cfg.verbose and (epoch % 10 == 0 or epoch == 1):\n",
    "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            print(f\"Epoch {epoch:03d} | train {train_loss:.6f} | val {val_loss:.6f} | lr {cur_lr:.2e}\")\n",
    "\n",
    "        if val_loss + 1e-12 < best_val:\n",
    "            best_val = val_loss\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= cfg.patience:\n",
    "                if cfg.verbose:\n",
    "                    print(f\"Early stop at epoch {epoch}, best_val={best_val:.6f}\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model, history\n",
    "\n",
    "def predict_logits(model: nn.Module, X: np.ndarray, batch_size: int = 256) -> np.ndarray:\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            xb = torch.from_numpy(X[i:i + batch_size].astype(np.float32)).to(device)\n",
    "            logits = model(xb)\n",
    "            outputs.append(logits.cpu().numpy())\n",
    "    return np.concatenate(outputs, axis=0)\n",
    "\n",
    "def softmax_np(logits: np.ndarray) -> np.ndarray:\n",
    "    logits = logits - logits.max(axis=1, keepdims=True)\n",
    "    exp = np.exp(logits)\n",
    "    return exp / exp.sum(axis=1, keepdims=True)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. 固定窗口交叉验证（多类别版本）\n",
    "\n",
    "# %%\n",
    "def fixed_window_cv_lstm_classifier(\n",
    "    X_seq_all: np.ndarray,\n",
    "    y_all: np.ndarray,\n",
    "    returns_all: np.ndarray,\n",
    "    seq_index: pd.DatetimeIndex,\n",
    "    test_size: int,\n",
    "    train_window: int,\n",
    "    val_size: int,\n",
    "    step_size: Optional[int],\n",
    "    cfg: TrainConfig,\n",
    "    collect_predictions: bool = False,\n",
    "    verbose: bool = False,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    if step_size is None:\n",
    "        step_size = test_size\n",
    "    N = len(X_seq_all)\n",
    "    if N < train_window + val_size + test_size:\n",
    "        raise ValueError(\"样本长度不足以执行指定窗口。\")\n",
    "\n",
    "    metrics = []\n",
    "    preds_list = []\n",
    "    start_index = train_window + val_size\n",
    "    fold_id = 0\n",
    "\n",
    "    for test_start in range(start_index, N - test_size + 1, step_size):\n",
    "        train_end = test_start\n",
    "        val_start = train_end - val_size\n",
    "        train_start = val_start - train_window\n",
    "        test_end = test_start + test_size\n",
    "\n",
    "        if train_start < 0:\n",
    "            continue\n",
    "\n",
    "        X_train = X_seq_all[train_start:val_start]\n",
    "        y_train = y_all[train_start:val_start]\n",
    "        X_val = X_seq_all[val_start:train_end]\n",
    "        y_val = y_all[val_start:train_end]\n",
    "        X_test = X_seq_all[test_start:test_end]\n",
    "        y_test = y_all[test_start:test_end]\n",
    "        r_test = returns_all[test_start:test_end]\n",
    "        test_dates = seq_index[test_start:test_end]\n",
    "\n",
    "        mean, std = fit_scaler_3d(X_train)\n",
    "        X_train_sc = transform_3d(X_train, mean, std)\n",
    "        X_val_sc = transform_3d(X_val, mean, std)\n",
    "        X_test_sc = transform_3d(X_test, mean, std)\n",
    "\n",
    "        train_loader = make_loader_classification(X_train_sc, y_train, cfg.batch_size, shuffle=True)\n",
    "        val_loader = make_loader_classification(X_val_sc, y_val, cfg.batch_size, shuffle=False)\n",
    "\n",
    "        model = LSTMMultiClassifier(input_size=X_train_sc.shape[-1], hidden_size=64, dropout=0.3)\n",
    "        model, _ = train_classifier(model, train_loader, val_loader, cfg)\n",
    "\n",
    "        logits_test = predict_logits(model, X_test_sc, batch_size=256)\n",
    "        proba_test = softmax_np(logits_test)\n",
    "        y_pred = proba_test.argmax(axis=1)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        try:\n",
    "            bacc = balanced_accuracy_score(y_test, y_pred)\n",
    "        except Exception:\n",
    "            bacc = np.nan\n",
    "        macro_f1 = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "        fold_metrics = {\n",
    "            \"fold\": fold_id,\n",
    "            \"acc\": acc,\n",
    "            \"bacc\": bacc,\n",
    "            \"macro_f1\": macro_f1,\n",
    "            \"train_size\": len(X_train_sc),\n",
    "            \"val_size\": len(X_val_sc),\n",
    "            \"test_size\": len(X_test_sc),\n",
    "            \"test_start\": test_dates[0],\n",
    "            \"test_end\": test_dates[-1],\n",
    "        }\n",
    "        metrics.append(fold_metrics)\n",
    "\n",
    "        if collect_predictions:\n",
    "            fold_pred = pd.DataFrame({\n",
    "                \"fold\": fold_id,\n",
    "                \"date\": test_dates,\n",
    "                \"actual_class\": y_test,\n",
    "                \"pred_class\": y_pred,\n",
    "                \"actual_return\": r_test,\n",
    "            })\n",
    "            for cls in range(NUM_CLASSES):\n",
    "                fold_pred[f\"proba_{cls}\"] = proba_test[:, cls]\n",
    "            preds_list.append(fold_pred)\n",
    "\n",
    "        fold_id += 1\n",
    "        del model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    preds_df = pd.concat(preds_list, ignore_index=True) if (collect_predictions and preds_list) else pd.DataFrame()\n",
    "    if verbose:\n",
    "        print(f\"共生成 {len(metrics_df)} 个折。\")\n",
    "    return metrics_df, preds_df\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9. 阈值扫描与选择\n",
    "\n",
    "# %%\n",
    "def run_threshold_search(\n",
    "    feat_df: pd.DataFrame,\n",
    "    feature_cols: List[str],\n",
    "    lookback: int,\n",
    "    thresholds: List[float],\n",
    "    train_window: int,\n",
    "    val_size: int,\n",
    "    test_size: int,\n",
    "    step_size: Optional[int],\n",
    "    cfg: TrainConfig,\n",
    "    min_class_count: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    records = []\n",
    "    for thr in thresholds:\n",
    "        labeled_df = feat_df.copy()\n",
    "        labeled_df[\"target_class\"] = label_significant_moves(labeled_df[\"next_return\"], thr)\n",
    "\n",
    "        class_counts = labeled_df[\"target_class\"].value_counts()\n",
    "        class_counts = class_counts.reindex([CLASS_ID_DOWN, CLASS_ID_NEUTRAL, CLASS_ID_UP], fill_value=0)\n",
    "        if class_counts.min() < min_class_count:\n",
    "            continue\n",
    "\n",
    "        X_seq, labels_dict, seq_index = make_sequences(\n",
    "            labeled_df,\n",
    "            feature_cols=feature_cols,\n",
    "            lookback=lookback,\n",
    "            label_cols=[\"target_class\", \"next_return\"]\n",
    "        )\n",
    "        y_all = labels_dict[\"target_class\"].astype(np.int64)\n",
    "        returns_all = labels_dict[\"next_return\"].astype(np.float32)\n",
    "\n",
    "        try:\n",
    "            cv_metrics, _ = fixed_window_cv_lstm_classifier(\n",
    "                X_seq, y_all, returns_all, seq_index,\n",
    "                test_size=test_size,\n",
    "                train_window=train_window,\n",
    "                val_size=val_size,\n",
    "                step_size=step_size,\n",
    "                cfg=cfg,\n",
    "                collect_predictions=False,\n",
    "                verbose=False\n",
    "            )\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        if cv_metrics.empty:\n",
    "            continue\n",
    "\n",
    "        record = {\n",
    "            \"threshold\": thr,\n",
    "            \"mean_acc\": cv_metrics[\"acc\"].mean(),\n",
    "            \"std_acc\": cv_metrics[\"acc\"].std(ddof=0),\n",
    "            \"mean_bacc\": cv_metrics[\"bacc\"].mean(),\n",
    "            \"mean_macro_f1\": cv_metrics[\"macro_f1\"].mean(),\n",
    "            \"folds\": len(cv_metrics)\n",
    "        }\n",
    "        records.append(record)\n",
    "\n",
    "    results_df = pd.DataFrame(records)\n",
    "    if not results_df.empty:\n",
    "        results_df = results_df.sort_values(\"mean_acc\", ascending=False).reset_index(drop=True)\n",
    "    return results_df\n",
    "\n",
    "def select_best_threshold(results_df: pd.DataFrame, desired_acc_range: Tuple[float, float] = (0.65, 0.70)) -> pd.Series:\n",
    "    if results_df.empty:\n",
    "        raise ValueError(\"阈值搜索无可用结果，请调整阈值范围或窗口参数。\")\n",
    "    lower, upper = desired_acc_range\n",
    "    in_range = results_df[(results_df[\"mean_acc\"] >= lower) & (results_df[\"mean_acc\"] <= upper)]\n",
    "    if not in_range.empty:\n",
    "        best = in_range.sort_values([\"mean_macro_f1\", \"mean_acc\"], ascending=False).iloc[0]\n",
    "    else:\n",
    "        center = (lower + upper) / 2\n",
    "        best = results_df.iloc[(results_df[\"mean_acc\"] - center).abs().argsort()].iloc[0]\n",
    "    return best\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10. 分类结果汇总工具\n",
    "\n",
    "# %%\n",
    "def summarize_classification(preds_df: pd.DataFrame, class_names: List[str]) -> Dict[str, object]:\n",
    "    y_true = preds_df[\"actual_class\"].astype(int).values\n",
    "    y_pred = preds_df[\"pred_class\"].astype(int).values\n",
    "\n",
    "    overall = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(y_true, y_pred),\n",
    "        \"macro_f1\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    }\n",
    "    report = classification_report(\n",
    "        y_true, y_pred,\n",
    "        labels=[CLASS_ID_DOWN, CLASS_ID_NEUTRAL, CLASS_ID_UP],\n",
    "        target_names=class_names,\n",
    "        digits=3,\n",
    "        zero_division=0\n",
    "    )\n",
    "    conf = confusion_matrix(\n",
    "        y_true, y_pred,\n",
    "        labels=[CLASS_ID_DOWN, CLASS_ID_NEUTRAL, CLASS_ID_UP]\n",
    "    )\n",
    "    conf_df = pd.DataFrame(conf, index=class_names, columns=class_names)\n",
    "\n",
    "    overall[\"classification_report\"] = report\n",
    "    overall[\"confusion_matrix\"] = conf_df\n",
    "    return overall\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 11. 交易策略模拟（显著上涨做多 / 显著下跌做空）\n",
    "\n",
    "# %%\n",
    "def simulate_significant_strategy(\n",
    "    preds_df: pd.DataFrame,\n",
    "    transaction_cost_bp: float = 2.0,\n",
    "    class_up: int = CLASS_ID_UP,\n",
    "    class_down: int = CLASS_ID_DOWN,\n",
    ") -> Tuple[pd.DataFrame, Dict[str, object]]:\n",
    "    if preds_df.empty:\n",
    "        raise ValueError(\"预测结果为空，无法回测策略。\")\n",
    "\n",
    "    df = preds_df.sort_values(\"date\").copy()\n",
    "    df[\"position\"] = 0\n",
    "    df.loc[df[\"pred_class\"] == class_up, \"position\"] = 1\n",
    "    df.loc[df[\"pred_class\"] == class_down, \"position\"] = -1\n",
    "\n",
    "    df[\"simple_return\"] = np.exp(df[\"actual_return\"]) - 1.0\n",
    "    df[\"strategy_simple_return\"] = df[\"position\"] * df[\"simple_return\"]\n",
    "\n",
    "    position_change = df[\"position\"].diff().abs()\n",
    "    position_change.iloc[0] = abs(df[\"position\"].iloc[0])\n",
    "    cost_rate = transaction_cost_bp / 10000.0\n",
    "    df[\"transaction_cost\"] = position_change * cost_rate\n",
    "    df[\"strategy_simple_return_after_cost\"] = df[\"strategy_simple_return\"] - df[\"transaction_cost\"]\n",
    "\n",
    "    df[\"strategy_equity\"] = (1 + df[\"strategy_simple_return_after_cost\"]).cumprod()\n",
    "    df[\"buyhold_equity\"] = (1 + df[\"simple_return\"]).cumprod()\n",
    "\n",
    "    df[\"strategy_equity\"] = df[\"strategy_equity\"].replace([np.inf, -np.inf], np.nan).fillna(method=\"ffill\")\n",
    "    df[\"buyhold_equity\"] = df[\"buyhold_equity\"].replace([np.inf, -np.inf], np.nan).fillna(method=\"ffill\")\n",
    "\n",
    "    stats: Dict[str, object] = {}\n",
    "    total_days = len(df)\n",
    "    if total_days == 0:\n",
    "        raise ValueError(\"无交易样本。\")\n",
    "\n",
    "    strategy_final = df[\"strategy_equity\"].iloc[-1]\n",
    "    buyhold_final = df[\"buyhold_equity\"].iloc[-1]\n",
    "    stats[\"total_return\"] = strategy_final - 1\n",
    "    stats[\"buyhold_total_return\"] = buyhold_final - 1\n",
    "\n",
    "    years = max(total_days / 252, 1 / 252)\n",
    "    stats[\"annualized_return\"] = strategy_final ** (1 / years) - 1\n",
    "    stats[\"buyhold_annual_return\"] = buyhold_final ** (1 / years) - 1\n",
    "\n",
    "    daily_mean = df[\"strategy_simple_return_after_cost\"].mean()\n",
    "    daily_std = df[\"strategy_simple_return_after_cost\"].std()\n",
    "    stats[\"annualized_volatility\"] = daily_std * np.sqrt(252)\n",
    "    stats[\"sharpe_ratio\"] = (daily_mean / daily_std) * np.sqrt(252) if daily_std > 1e-8 else np.nan\n",
    "\n",
    "    cum_max = df[\"strategy_equity\"].cummax()\n",
    "    drawdown = df[\"strategy_equity\"] / cum_max - 1\n",
    "    stats[\"max_drawdown\"] = drawdown.min()\n",
    "\n",
    "    trade_mask = df[\"position\"] != 0\n",
    "    if trade_mask.any():\n",
    "        stats[\"hit_rate_on_trades\"] = (df.loc[trade_mask, \"actual_class\"] == df.loc[trade_mask, \"pred_class\"]).mean()\n",
    "        stats[\"avg_trade_return\"] = df.loc[trade_mask, \"strategy_simple_return_after_cost\"].mean()\n",
    "        stats[\"num_trading_days\"] = int(trade_mask.sum())\n",
    "    else:\n",
    "        stats[\"hit_rate_on_trades\"] = np.nan\n",
    "        stats[\"avg_trade_return\"] = np.nan\n",
    "        stats[\"num_trading_days\"] = 0\n",
    "\n",
    "    pos_dist = df[\"position\"].value_counts(normalize=True).sort_index()\n",
    "    stats[\"position_distribution\"] = {\n",
    "        \"short_pct\": pos_dist.get(-1, 0.0),\n",
    "        \"flat_pct\": pos_dist.get(0, 0.0),\n",
    "        \"long_pct\": pos_dist.get(1, 0.0),\n",
    "    }\n",
    "    stats[\"transaction_cost_bp\"] = transaction_cost_bp\n",
    "\n",
    "    return df, stats\n",
    "\n",
    "def plot_equity_curves(sim_df: pd.DataFrame, threshold: float, class_names: List[str]):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True, gridspec_kw={\"height_ratios\": [3, 1]})\n",
    "\n",
    "    ax_price = axes[0]\n",
    "    ax_pos = axes[1]\n",
    "\n",
    "    ax_price.plot(sim_df[\"date\"], sim_df[\"strategy_equity\"], label=\"策略权益\", color=\"tab:blue\", linewidth=2)\n",
    "    ax_price.plot(sim_df[\"date\"], sim_df[\"buyhold_equity\"], label=\"买入持有权益\", color=\"tab:orange\", linestyle=\"--\")\n",
    "    ax_price.set_ylabel(\"累计净值\")\n",
    "    ax_price.set_title(f\"策略 vs 买入持有（阈值 = {threshold:.2%}）\")\n",
    "    ax_price.legend(loc=\"best\")\n",
    "    ax_price.grid(True, alpha=0.3)\n",
    "\n",
    "    ax_pos.step(sim_df[\"date\"], sim_df[\"position\"], where=\"post\", color=\"tab:green\", linewidth=1.5)\n",
    "    ax_pos.set_ylabel(\"仓位\")\n",
    "    ax_pos.set_xlabel(\"日期\")\n",
    "    ax_pos.set_yticks([-1, 0, 1])\n",
    "    ax_pos.set_yticklabels([\"做空\", \"空仓\", \"做多\"])\n",
    "    ax_pos.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 12. 参数设定与阈值搜索\n",
    "\n",
    "# %%\n",
    "LOOKBACK = 14\n",
    "TRAIN_WINDOW = 150\n",
    "VAL_SIZE = 10\n",
    "TEST_SIZE = 7\n",
    "STEP_SIZE = 7\n",
    "THRESHOLD_GRID = np.round(np.linspace(0.004, 0.018, 12), 4)  # 0.4% ~ 1.8%\n",
    "\n",
    "threshold_results = run_threshold_search(\n",
    "    feat_df=feat_df,\n",
    "    feature_cols=feature_cols,\n",
    "    lookback=LOOKBACK,\n",
    "    thresholds=THRESHOLD_GRID.tolist(),\n",
    "    train_window=TRAIN_WINDOW,\n",
    "    val_size=VAL_SIZE,\n",
    "    test_size=TEST_SIZE,\n",
    "    step_size=STEP_SIZE,\n",
    "    cfg=CFG,\n",
    "    min_class_count=5,\n",
    ")\n",
    "\n",
    "print(\"阈值搜索结果（按准确率排序）:\")\n",
    "display(threshold_results)\n",
    "\n",
    "best_threshold_info = select_best_threshold(threshold_results, desired_acc_range=(0.65, 0.70))\n",
    "best_threshold = float(best_threshold_info[\"threshold\"])\n",
    "print(f\"\\n选定阈值: {best_threshold:.2%}\")\n",
    "print(best_threshold_info)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 13. 使用最佳阈值重新训练并收集 OOF 预测\n",
    "\n",
    "# %%\n",
    "feat_df_best = feat_df.copy()\n",
    "feat_df_best[\"target_class\"] = label_significant_moves(feat_df_best[\"next_return\"], best_threshold)\n",
    "\n",
    "X_seq_best, label_dict_best, seq_index_best = make_sequences(\n",
    "    feat_df_best,\n",
    "    feature_cols=feature_cols,\n",
    "    lookback=LOOKBACK,\n",
    "    label_cols=[\"target_class\", \"next_return\"]\n",
    ")\n",
    "\n",
    "y_best = label_dict_best[\"target_class\"].astype(np.int64)\n",
    "returns_best = label_dict_best[\"next_return\"].astype(np.float32)\n",
    "\n",
    "cv_metrics, cv_preds = fixed_window_cv_lstm_classifier(\n",
    "    X_seq_best, y_best, returns_best, seq_index_best,\n",
    "    test_size=TEST_SIZE,\n",
    "    train_window=TRAIN_WINDOW,\n",
    "    val_size=VAL_SIZE,\n",
    "    step_size=STEP_SIZE,\n",
    "    cfg=CFG,\n",
    "    collect_predictions=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"交叉验证折指标（前 5 行）:\")\n",
    "display(cv_metrics.head())\n",
    "\n",
    "summary_table = cv_metrics[[\"acc\", \"bacc\", \"macro_f1\"]].agg([\"mean\", \"std\", \"min\", \"max\"])\n",
    "print(\"\\n交叉验证指标汇总:\")\n",
    "display(summary_table)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 14. 分类表现汇总\n",
    "\n",
    "# %%\n",
    "class_names = [\n",
    "    f\"显著下跌 (≤ -{best_threshold:.2%})\",\n",
    "    \"无显著波动\",\n",
    "    f\"显著上涨 (≥ {best_threshold:.2%})\",\n",
    "]\n",
    "\n",
    "classification_summary = summarize_classification(cv_preds, class_names)\n",
    "print(\"整体准确率:\", f\"{classification_summary['accuracy']:.3f}\")\n",
    "print(\"整体平衡准确率:\", f\"{classification_summary['balanced_accuracy']:.3f}\")\n",
    "print(\"Macro-F1:\", f\"{classification_summary['macro_f1']:.3f}\")\n",
    "print(\"\\n分类报告:\")\n",
    "print(classification_summary[\"classification_report\"])\n",
    "print(\"混淆矩阵:\")\n",
    "display(classification_summary[\"confusion_matrix\"])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 15. 策略回测\n",
    "\n",
    "# %%\n",
    "TRANSACTION_COST_BP = 2.0  # 默认双边 2 bps\n",
    "sim_df, perf_stats = simulate_significant_strategy(\n",
    "    cv_preds,\n",
    "    transaction_cost_bp=TRANSACTION_COST_BP,\n",
    "    class_up=CLASS_ID_UP,\n",
    "    class_down=CLASS_ID_DOWN\n",
    ")\n",
    "\n",
    "print(\"策略样本预览:\")\n",
    "display(sim_df.head())\n",
    "\n",
    "print(\"\\n策略表现指标:\")\n",
    "perf_table = pd.DataFrame([{\n",
    "    \"总收益\": perf_stats[\"total_return\"],\n",
    "    \"买入持有总收益\": perf_stats[\"buyhold_total_return\"],\n",
    "    \"年化收益率\": perf_stats[\"annualized_return\"],\n",
    "    \"买入持有年化收益率\": perf_stats[\"buyhold_annual_return\"],\n",
    "    \"年化波动率\": perf_stats[\"annualized_volatility\"],\n",
    "    \"夏普率\": perf_stats[\"sharpe_ratio\"],\n",
    "    \"最大回撤\": perf_stats[\"max_drawdown\"],\n",
    "    \"交易日数\": perf_stats[\"num_trading_days\"],\n",
    "    \"交易准确率\": perf_stats[\"hit_rate_on_trades\"],\n",
    "    \"单次交易平均收益\": perf_stats[\"avg_trade_return\"],\n",
    "    \"多头占比\": perf_stats[\"position_distribution\"][\"long_pct\"],\n",
    "    \"空头占比\": perf_stats[\"position_distribution\"][\"short_pct\"],\n",
    "    \"空仓占比\": perf_stats[\"position_distribution\"][\"flat_pct\"],\n",
    "    \"交易成本 (bps)\": perf_stats[\"transaction_cost_bp\"],\n",
    "}])\n",
    "display(perf_table.T.rename(columns={0: \"值\"}))\n",
    "\n",
    "plot_equity_curves(sim_df, threshold=best_threshold, class_names=class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cwq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
