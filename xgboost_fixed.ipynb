{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a48e3b0",
   "metadata": {},
   "source": [
    "# (Legacy) xgboost_fixed.ipynb\n",
    "\n",
    "XGBoost baseline 的固定窗口 CV 版本。若要纳入报告对比，建议后续也迁移为脚本化/模块化输出到 `outputs/reports/`。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073306d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Fixed-window time-series modeling with XGBoost\n",
    "#\n",
    "# This notebook implements a strict fixed-window time-series cross-validation (CV):\n",
    "# - Each fold trains only on the most recent `train_window` observations (e.g., 20 trading days),\n",
    "# - Validates on the next `val_size` days (for early stopping),\n",
    "# - And tests on the subsequent `test_size` days.\n",
    "#\n",
    "# Compared to expanding windows, this enforces the assumption that the target depends only on the most recent window of history.\n",
    "# Feature engineering is also constrained to use at most 20-day lookback windows (no long-memory EMAs).\n",
    "#\n",
    "# Assumptions:\n",
    "# - We predict next-day return from the information available at the close of day t.\n",
    "# - If you execute at the next day open/close, ensure your operational assumptions match this data availability.\n",
    "\n",
    "\n",
    "# %%\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid') if 'seaborn-v0_8-whitegrid' in plt.style.available else plt.style.use('ggplot')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, roc_auc_score,\n",
    "    mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "\n",
    "# Assume XGBoost is available (no fallback)\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "print(\"XGBoost available: True\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Data: 2800.HK, daily, 3 years\n",
    "# Pull adjusted OHLCV data using `yfinance`. If `period` fails, fallback to explicit start/end dates.\n",
    "\n",
    "\n",
    "# %%\n",
    "def fetch_prices(ticker: str = \"2800.HK\", period: str = \"3y\", interval: str = \"1d\") -> pd.DataFrame:\n",
    "    \"\"\"Fetch adjusted OHLCV data. Index is timezone-naive DateTimeIndex.\"\"\"\n",
    "    tk = yf.Ticker(ticker)\n",
    "    df = tk.history(period=period, interval=interval, auto_adjust=True)\n",
    "    if df is None or df.empty:\n",
    "        end = pd.Timestamp.today().normalize()\n",
    "        start = end - pd.Timedelta(days=3*370)\n",
    "        df = tk.history(start=start, end=end, interval=interval, auto_adjust=True)\n",
    "    if df is None or df.empty:\n",
    "        raise RuntimeError(f\"Cannot fetch data for {ticker}\")\n",
    "    df.index = pd.to_datetime(df.index).tz_localize(None)\n",
    "    return df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].copy()\n",
    "\n",
    "ticker = \"2800.HK\"\n",
    "df_raw = fetch_prices(ticker=ticker, period=\"3y\", interval=\"1d\")\n",
    "print(f\"Data range: {df_raw.index.min().date()} to {df_raw.index.max().date()}\")\n",
    "print(f\"Total samples: {len(df_raw)}\")\n",
    "df_raw.tail()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Feature engineering (max 20-day lookback)\n",
    "# - Use only past information up to and including time t (no leakage).\n",
    "# - Constrain all rolling windows to ≤ 20 trading days to match the \"depends only on last 20 days\" assumption.\n",
    "# - Labels:\n",
    "#   - Regression: `next_return` = log return from t to t+1 (set `use_log_return=True`).\n",
    "#   - Classification: `target_up` = 1 if `next_return` > 0 else 0.\n",
    "\n",
    "\n",
    "# %%\n",
    "def calculate_rsi_rolling(series: pd.Series, period: int = 14) -> pd.Series:\n",
    "    \"\"\"RSI computed with rolling simple means to ensure finite memory.\n",
    "    Uses only the last `period` observations.\n",
    "    \"\"\"\n",
    "    delta = series.diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -delta.clip(upper=0)\n",
    "    roll_up = up.rolling(window=period, min_periods=period).mean()\n",
    "    roll_down = down.rolling(window=period, min_periods=period).mean().replace(0, np.nan)\n",
    "    rs = roll_up / roll_down\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def calculate_macd_sma(close: pd.Series, fast: int = 8, slow: int = 16, signal: int = 6):\n",
    "    \"\"\"MACD variant using simple moving averages to enforce finite lookback windows (≤ 20).\n",
    "    - macd_line = SMA_fast - SMA_slow\n",
    "    - signal_line = SMA of macd_line (window = signal)\n",
    "    - histogram = macd_line - signal_line\n",
    "    \"\"\"\n",
    "    sma_fast = close.rolling(window=fast, min_periods=fast).mean()\n",
    "    sma_slow = close.rolling(window=slow, min_periods=slow).mean()\n",
    "    macd_line = sma_fast - sma_slow\n",
    "    signal_line = macd_line.rolling(window=signal, min_periods=signal).mean()\n",
    "    histogram = macd_line - signal_line\n",
    "    return macd_line, signal_line, histogram\n",
    "\n",
    "def build_features_and_labels(df: pd.DataFrame,\n",
    "                              horizon: int = 1,\n",
    "                              use_log_return: bool = True,\n",
    "                              max_lookback: int = 20) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Build features using only up to `max_lookback` days of history.\n",
    "    All features are computed using information available by the end of day t,\n",
    "    and the label predicts the return from t to t+horizon.\n",
    "    \"\"\"\n",
    "    close = df[\"Close\"].copy()\n",
    "    vol = df[\"Volume\"].copy() if \"Volume\" in df.columns else None\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Returns up to t (these use close_t and close_{t-1})\n",
    "    features[\"ret_1d\"] = close.pct_change(1)\n",
    "    features[\"ret_2d\"] = close.pct_change(2)\n",
    "    features[\"ret_5d\"] = close.pct_change(5)\n",
    "    features[\"ret_10d\"] = close.pct_change(10)\n",
    "\n",
    "    # Realized volatility based on past returns (bounded by 20 days)\n",
    "    features[\"vol_5d\"] = features[\"ret_1d\"].rolling(5, min_periods=5).std()\n",
    "    features[\"vol_10d\"] = features[\"ret_1d\"].rolling(10, min_periods=10).std()\n",
    "    features[\"vol_20d\"] = features[\"ret_1d\"].rolling(20, min_periods=20).std()\n",
    "\n",
    "    # Moving averages and relative position (all computed with ≤ 20-day windows)\n",
    "    sma_5 = close.rolling(5, min_periods=5).mean()\n",
    "    sma_10 = close.rolling(10, min_periods=10).mean()\n",
    "    sma_20 = close.rolling(20, min_periods=20).mean()\n",
    "\n",
    "    # Relative distance of today's close to MAs\n",
    "    features[\"close_to_sma5\"] = close / sma_5 - 1\n",
    "    features[\"close_to_sma10\"] = close / sma_10 - 1\n",
    "    features[\"close_to_sma20\"] = close / sma_20 - 1\n",
    "    features[\"sma5_sma10\"] = sma_5 / sma_10 - 1\n",
    "    features[\"sma10_sma20\"] = sma_10 / sma_20 - 1\n",
    "\n",
    "    # Technical indicators with ≤ 20-day lookback\n",
    "    features[\"rsi_14\"] = calculate_rsi_rolling(close, 14)\n",
    "    macd_line, signal_line, histogram = calculate_macd_sma(close, fast=8, slow=16, signal=6)\n",
    "    features[\"macd\"] = macd_line\n",
    "    features[\"macd_signal\"] = signal_line\n",
    "    features[\"macd_hist\"] = histogram\n",
    "\n",
    "    # Bollinger bands (20-day)\n",
    "    bb_period = min(20, max_lookback)\n",
    "    bb_std = close.rolling(bb_period, min_periods=bb_period).std()\n",
    "    bb_mean = close.rolling(bb_period, min_periods=bb_period).mean()\n",
    "    features[\"bb_upper\"] = (bb_mean + 2 * bb_std - close) / close\n",
    "    features[\"bb_lower\"] = (close - (bb_mean - 2 * bb_std)) / close\n",
    "\n",
    "    # Volume features (bounded to 20-day windows)\n",
    "    if vol is not None:\n",
    "        features[\"volume_ratio\"] = vol / vol.rolling(20, min_periods=20).mean()\n",
    "        features[\"volume_change\"] = vol.pct_change()\n",
    "\n",
    "    # Calendar features (known at time t)\n",
    "    features[\"day_of_week\"] = df.index.dayofweek\n",
    "    features[\"month\"] = df.index.month\n",
    "    features[\"quarter\"] = df.index.quarter\n",
    "\n",
    "    # Lagged returns up to 20 days back\n",
    "    for lag in [1, 2, 3, 5, 10, 15, 20]:\n",
    "        features[f\"ret_lag_{lag}\"] = features[\"ret_1d\"].shift(lag)\n",
    "\n",
    "    # Targets: next-period return and direction\n",
    "    if use_log_return:\n",
    "        next_ret = np.log(close.shift(-horizon) / close)\n",
    "    else:\n",
    "        next_ret = close.pct_change(horizon).shift(-horizon)\n",
    "    target_up = (next_ret > 0).astype(int)\n",
    "\n",
    "    # Clean and merge\n",
    "    features = features.replace([np.inf, -np.inf], np.nan)\n",
    "    data = features.copy()\n",
    "    data[\"next_return\"] = next_ret\n",
    "    data[\"target_up\"] = target_up\n",
    "    data = data.dropna().copy()\n",
    "\n",
    "    feature_cols = [c for c in data.columns if c not in [\"next_return\", \"target_up\"]]\n",
    "    return data, feature_cols\n",
    "\n",
    "# Build features & labels (log-returns by default)\n",
    "feat_df, feature_cols = build_features_and_labels(df_raw, horizon=1, use_log_return=True, max_lookback=20)\n",
    "print(f\"Samples after cleaning: {len(feat_df)}, Features: {len(feature_cols)}\")\n",
    "feat_df.tail()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Fixed-window time-series CV (rolling window)\n",
    "# - For each fold:\n",
    "#   - Train on the most recent `train_window` samples ending at `test_start - val_size`.\n",
    "#   - Validate on the next `val_size` samples ending at `test_start` (for early stopping).\n",
    "#   - Test on the next `test_size` samples.\n",
    "# - Evaluate both classification (direction) and regression (next_return).\n",
    "\n",
    "\n",
    "# %%\n",
    "def train_models(X_train: np.ndarray, y_cls_train: np.ndarray, y_reg_train: np.ndarray,\n",
    "                 X_val: Optional[np.ndarray] = None,\n",
    "                 y_cls_val: Optional[np.ndarray] = None,\n",
    "                 y_reg_val: Optional[np.ndarray] = None):\n",
    "    \"\"\"Train classifier and regressor.\n",
    "    Uses early stopping only if a validation set is provided and non-empty.\n",
    "    \"\"\"\n",
    "    use_es = X_val is not None and y_cls_val is not None and len(X_val) > 0\n",
    "\n",
    "\n",
    "    clf_kwargs = dict(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=4,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        reg_alpha=0.5,\n",
    "        random_state=SEED,\n",
    "        eval_metric=\"logloss\",\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\"\n",
    "    )\n",
    "    if use_es:\n",
    "        clf_kwargs[\"early_stopping_rounds\"] = 30\n",
    "    clf = XGBClassifier(**clf_kwargs)\n",
    "\n",
    "    if use_es:\n",
    "        clf.fit(X_train, y_cls_train, eval_set=[(X_val, y_cls_val)], verbose=False)\n",
    "    else:\n",
    "        clf.fit(X_train, y_cls_train, verbose=False)\n",
    "\n",
    "    reg_kwargs = dict(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=4,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        reg_alpha=0.5,\n",
    "        random_state=SEED,\n",
    "        eval_metric=\"rmse\",\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\"\n",
    "    )\n",
    "    if use_es:\n",
    "        reg_kwargs[\"early_stopping_rounds\"] = 30\n",
    "    reg = XGBRegressor(**reg_kwargs)\n",
    "\n",
    "    if use_es:\n",
    "        reg.fit(X_train, y_reg_train, eval_set=[(X_val, y_reg_val)], verbose=False)\n",
    "    else:\n",
    "        reg.fit(X_train, y_reg_train, verbose=False)\n",
    "\n",
    "    return clf, reg\n",
    "\n",
    "def eval_fold(clf, reg, X_test: np.ndarray, y_cls_test: np.ndarray, y_reg_test: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate classification and regression on one fold.\"\"\"\n",
    "    proba = clf.predict_proba(X_test)[:, 1]\n",
    "    y_pred_cls = (proba >= 0.5).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_cls_test, y_pred_cls)\n",
    "    bacc = balanced_accuracy_score(y_cls_test, y_pred_cls)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_cls_test, proba)\n",
    "    except Exception:\n",
    "        auc = np.nan\n",
    "\n",
    "    y_pred_reg = reg.predict(X_test)\n",
    "    reg_mae = mean_absolute_error(y_reg_test, y_pred_reg)\n",
    "    reg_rmse = np.sqrt(mean_squared_error(y_reg_test, y_pred_reg))\n",
    "    reg_r2 = r2_score(y_reg_test, y_pred_reg)\n",
    "    dir_acc = (np.sign(y_reg_test) == np.sign(y_pred_reg)).mean()\n",
    "\n",
    "    return {\n",
    "        \"cls_acc\": acc, \"cls_bacc\": bacc, \"cls_auc\": auc,\n",
    "        \"reg_mae\": reg_mae, \"reg_rmse\": reg_rmse, \"reg_r2\": reg_r2,\n",
    "        \"reg_dir_acc\": dir_acc\n",
    "    }\n",
    "\n",
    "def fixed_window_cv(feat_df: pd.DataFrame, feature_cols: List[str],\n",
    "                    test_size: int = 14,\n",
    "                    train_window: int = 20,\n",
    "                    val_size: int = 5,\n",
    "                    step_size: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"Fixed-window CV: roll a constant-length training window forward through time.\n",
    "    - For each fold:\n",
    "      train = [test_start - val_size - train_window, test_start - val_size)\n",
    "      val   = [test_start - val_size, test_start)\n",
    "      test  = [test_start, test_start + test_size)\n",
    "    \"\"\"\n",
    "    if step_size is None:\n",
    "        step_size = test_size  # non-overlapping test windows by default\n",
    "\n",
    "    metrics = []\n",
    "    X_all = feat_df[feature_cols].astype(float).values\n",
    "    y_cls_all = feat_df[\"target_up\"].astype(int).values\n",
    "    y_reg_all = feat_df[\"next_return\"].astype(float).values\n",
    "\n",
    "    N = len(feat_df)\n",
    "    start_index = train_window + val_size\n",
    "    fold_id = 0\n",
    "\n",
    "    for test_start in range(start_index, N - test_size + 1, step_size):\n",
    "        train_end = test_start\n",
    "        val_start = train_end - val_size\n",
    "        train_start = val_start - train_window\n",
    "        test_end = test_start + test_size\n",
    "\n",
    "        if train_start < 0:\n",
    "            continue  # not enough history\n",
    "\n",
    "        X_train = X_all[train_start: val_start]\n",
    "        y_cls_train = y_cls_all[train_start: val_start]\n",
    "        y_reg_train = y_reg_all[train_start: val_start]\n",
    "\n",
    "        X_val = X_all[val_start: train_end] if val_size > 0 else None\n",
    "        y_cls_val = y_cls_all[val_start: train_end] if val_size > 0 else None\n",
    "        y_reg_val = y_reg_all[val_start: train_end] if val_size > 0 else None\n",
    "\n",
    "        X_test = X_all[test_start: test_end]\n",
    "        y_cls_test = y_cls_all[test_start: test_end]\n",
    "        y_reg_test = y_reg_all[test_start: test_end]\n",
    "\n",
    "        clf, reg = train_models(X_train, y_cls_train, y_reg_train, X_val, y_cls_val, y_reg_val)\n",
    "        fold_metrics = eval_fold(clf, reg, X_test, y_cls_test, y_reg_test)\n",
    "        fold_metrics[\"fold\"] = fold_id\n",
    "        fold_metrics[\"train_size\"] = len(X_train)\n",
    "        fold_metrics[\"val_size\"] = len(X_val) if X_val is not None else 0\n",
    "        fold_metrics[\"test_size\"] = len(X_test)\n",
    "        fold_metrics[\"test_start\"] = feat_df.index[test_start]\n",
    "        fold_metrics[\"test_end\"] = feat_df.index[test_end - 1]\n",
    "        metrics.append(fold_metrics)\n",
    "        fold_id += 1\n",
    "\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "# Run fixed-window CV (default: train_window=20, val_size=5, test_size=14)\n",
    "cv_results = fixed_window_cv(feat_df, feature_cols,\n",
    "                             test_size=14,\n",
    "                             train_window=30,\n",
    "                             val_size=10,\n",
    "                             step_size=14)\n",
    "\n",
    "print(f\"Folds: {len(cv_results)}\")\n",
    "cv_results[[\"fold\",\"test_start\",\"test_end\",\"train_size\",\"val_size\",\"test_size\"]]\n",
    "\n",
    "\n",
    "# %%\n",
    "def summarize_metrics(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    return df[cols].agg(['mean','std','min','max'])\n",
    "\n",
    "print(\"\\n=== Per-fold metrics (head) ===\")\n",
    "display(cv_results.head())\n",
    "\n",
    "print(\"\\n=== Summary (Classification) ===\")\n",
    "display(summarize_metrics(cv_results, [\"cls_acc\", \"cls_bacc\", \"cls_auc\"]))\n",
    "\n",
    "print(\"\\n=== Summary (Regression on next_return) ===\")\n",
    "display(summarize_metrics(cv_results, [\"reg_mae\", \"reg_rmse\", \"reg_r2\", \"reg_dir_acc\"]))\n",
    "\n",
    "# Naive baselines over the full sample\n",
    "naive_mae = feat_df[\"next_return\"].abs().mean()              # Predict 0 for regression\n",
    "naive_dir_acc = max((feat_df[\"next_return\"] > 0).mean(),     # Predict majority sign for direction\n",
    "                    (feat_df[\"next_return\"] <= 0).mean())\n",
    "\n",
    "print(\"\\n=== Naive baselines over full sample ===\")\n",
    "print(f\"Regression MAE baseline (predict 0): {naive_mae:.6f}\")\n",
    "print(f\"Direction baseline (majority sign): {naive_dir_acc:.3f}\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Plot predictions for one fold\n",
    "# - Subplot 1: `next_return` (actual vs predicted) within the test window.\n",
    "# - Subplot 2: Reconstructed price paths (actual vs predicted) starting from the previous close.\n",
    "# - Set `assume_log_return=True` if you built labels using log returns (default here).\n",
    "\n",
    "\n",
    "# %%\n",
    "def pick_fold_index(cv_df: pd.DataFrame, mode: str = 'last') -> int:\n",
    "    if len(cv_df) == 0:\n",
    "        raise ValueError(\"cv_results is empty. Run fixed-window CV first.\")\n",
    "    if mode == 'last':\n",
    "        return int(cv_df.iloc[-1]['fold'])\n",
    "    elif mode == 'best_auc':\n",
    "        idx = cv_df['cls_auc'].idxmax()\n",
    "        return int(cv_df.loc[idx, 'fold'])\n",
    "    elif mode == 'best_dir':\n",
    "        idx = cv_df['reg_dir_acc'].idxmax()\n",
    "        return int(cv_df.loc[idx, 'fold'])\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'last' | 'best_auc' | 'best_dir'\")\n",
    "\n",
    "def reconstruct_price_path(start_price: float, returns: np.ndarray, assume_log_return: bool = True) -> np.ndarray:\n",
    "    if assume_log_return:\n",
    "        path = start_price * np.exp(np.cumsum(returns))\n",
    "    else:\n",
    "        path = start_price * np.cumprod(1.0 + returns)\n",
    "    return path\n",
    "\n",
    "def plot_fold_predictions_fixed(feat_df: pd.DataFrame, feature_cols: List[str], df_raw: pd.DataFrame,\n",
    "                                cv_results: pd.DataFrame,\n",
    "                                fold_to_plot: str = 'last', assume_log_return: bool = True):\n",
    "    # 1) Choose fold\n",
    "    fold_idx = pick_fold_index(cv_results, mode=fold_to_plot)\n",
    "    row = cv_results[cv_results['fold'] == fold_idx].iloc[0]\n",
    "    test_start_date = row['test_start']\n",
    "    test_end_date = row['test_end']\n",
    "\n",
    "    # 2) Map dates back to positional indices\n",
    "    all_dates = feat_df.index\n",
    "    test_start = all_dates.get_loc(test_start_date)\n",
    "    test_end = all_dates.get_loc(test_end_date) + 1  # right-open\n",
    "\n",
    "    # Reconstruct train/val positions from sizes stored in cv_results\n",
    "    train_size = int(row['train_size'])\n",
    "    val_size = int(row['val_size'])\n",
    "    train_end = test_start\n",
    "    val_start = train_end - val_size\n",
    "    train_start = val_start - train_size\n",
    "\n",
    "    X_all = feat_df[feature_cols].astype(float).values\n",
    "    y_cls_all = feat_df[\"target_up\"].astype(int).values\n",
    "    y_reg_all = feat_df[\"next_return\"].astype(float).values\n",
    "\n",
    "    X_train = X_all[train_start: val_start]\n",
    "    y_cls_train = y_cls_all[train_start: val_start]\n",
    "    y_reg_train = y_reg_all[train_start: val_start]\n",
    "\n",
    "    X_val = X_all[val_start: train_end] if val_size > 0 else None\n",
    "    y_cls_val = y_cls_all[val_start: train_end] if val_size > 0 else None\n",
    "    y_reg_val = y_reg_all[val_start: train_end] if val_size > 0 else None\n",
    "\n",
    "    X_test = X_all[test_start: test_end]\n",
    "    y_cls_test = y_cls_all[test_start: test_end]\n",
    "    y_reg_test = y_reg_all[test_start: test_end]\n",
    "    test_dates = all_dates[test_start: test_end]\n",
    "\n",
    "    # 3) Train and predict\n",
    "    clf, reg = train_models(X_train, y_cls_train, y_reg_train, X_val, y_cls_val, y_reg_val)\n",
    "    proba = clf.predict_proba(X_test)[:, 1]\n",
    "    y_pred_cls = (proba >= 0.5).astype(int)\n",
    "    y_pred_reg = reg.predict(X_test)\n",
    "\n",
    "    # 4) Plot returns and reconstructed price paths\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "    ax1, ax2 = axes\n",
    "\n",
    "    ax1.plot(test_dates, y_reg_test, label='Actual next_return', color='tab:blue')\n",
    "    ax1.plot(test_dates, y_pred_reg, label='Predicted next_return', color='tab:orange', alpha=0.8)\n",
    "    ax1.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax1.set_title(f'Fold {fold_idx}: next_return - Actual vs Predicted')\n",
    "    ax1.set_ylabel('Return')\n",
    "    ax1.legend(loc='best')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Reconstruct price path from previous close (aligned to feat_df dates)\n",
    "    try:\n",
    "        raw_close_aligned = df_raw.loc[feat_df.index, 'Close']\n",
    "        price_start_idx = test_start - 1\n",
    "        if price_start_idx >= 0:\n",
    "            start_close = float(raw_close_aligned.iloc[price_start_idx])\n",
    "            actual_price_path = reconstruct_price_path(start_close, y_reg_test, assume_log_return)\n",
    "            pred_price_path = reconstruct_price_path(start_close, y_pred_reg, assume_log_return)\n",
    "            price_index = [all_dates[price_start_idx]] + list(test_dates)\n",
    "            ax2.plot(price_index, [start_close] + list(actual_price_path), label='Actual price (reconstructed)', color='tab:green')\n",
    "            ax2.plot(price_index, [start_close] + list(pred_price_path), label='Predicted price (reconstructed)', color='tab:red', alpha=0.8)\n",
    "            ax2.set_title(f'Fold {fold_idx}: Price path (from previous close)')\n",
    "            ax2.set_ylabel('Price')\n",
    "            ax2.legend(loc='best')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            ax2.set_visible(False)\n",
    "    except Exception:\n",
    "        ax2.set_visible(False)\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 5) Print fold metrics\n",
    "    dir_acc = (np.sign(y_reg_test) == np.sign(y_pred_reg)).mean()\n",
    "    print(f\"Fold {fold_idx} metrics:\")\n",
    "    print(f\"  Classification - Acc: {accuracy_score(y_cls_test, y_pred_cls):.3f}, \"\n",
    "          f\"BalancedAcc: {balanced_accuracy_score(y_cls_test, y_pred_cls):.3f}, \"\n",
    "          f\"AUC: {roc_auc_score(y_cls_test, proba):.3f}\")\n",
    "    print(f\"  Regression - MAE: {mean_absolute_error(y_reg_test, y_pred_reg):.6f}, \"\n",
    "          f\"R2: {r2_score(y_reg_test, y_pred_reg):.6f}, \"\n",
    "          f\"DirAcc: {dir_acc:.3f}\")\n",
    "\n",
    "# Plot last fold by default; use 'best_auc' or 'best_dir' to choose alternative folds\n",
    "plot_fold_predictions_fixed(\n",
    "    feat_df=feat_df,\n",
    "    feature_cols=feature_cols,\n",
    "    df_raw=df_raw,\n",
    "    cv_results=cv_results,\n",
    "    fold_to_plot='last',   # 'last' | 'best_auc' | 'best_dir'\n",
    "    assume_log_return=True # Set False if you used simple returns\n",
    ")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Out-of-fold (OOF) classification using fixed-window CV\n",
    "# Generate OOF probabilities for the classification task across all folds and compute global metrics.\n",
    "\n",
    "\n",
    "# %%\n",
    "def oof_fixed_window(feat_df: pd.DataFrame, feature_cols: List[str],\n",
    "                     test_size: int = 7, train_window: int = 20, val_size: int = 5, step_size: Optional[int] = None):\n",
    "    if step_size is None:\n",
    "        step_size = test_size\n",
    "    X_all = feat_df[feature_cols].astype(float).values\n",
    "    y_cls_all = feat_df[\"target_up\"].astype(int).values\n",
    "    N = len(feat_df)\n",
    "\n",
    "    oof_true, oof_proba, oof_pred = [], [], []\n",
    "\n",
    "    start_index = train_window + val_size\n",
    "    for test_start in range(start_index, N - test_size + 1, step_size):\n",
    "        train_end = test_start\n",
    "        val_start = train_end - val_size\n",
    "        train_start = val_start - train_window\n",
    "        test_end = test_start + test_size\n",
    "        if train_start < 0:\n",
    "            continue\n",
    "\n",
    "        X_train = X_all[train_start: val_start]\n",
    "        y_train_cls = y_cls_all[train_start: val_start]\n",
    "        X_val = X_all[val_start: train_end] if val_size > 0 else None\n",
    "        y_val_cls = y_cls_all[val_start: train_end] if val_size > 0 else None\n",
    "\n",
    "        # Train classifier only (regressor unused here)\n",
    "        clf, _ = train_models(X_train, y_train_cls, y_train_cls, X_val, y_val_cls, y_val_cls)\n",
    "        proba = clf.predict_proba(X_all[test_start: test_end])[:, 1]\n",
    "        pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "        oof_true.append(y_cls_all[test_start: test_end])\n",
    "        oof_proba.append(proba)\n",
    "        oof_pred.append(pred)\n",
    "\n",
    "    y_true = np.concatenate(oof_true)\n",
    "    y_proba = np.concatenate(oof_proba)\n",
    "    y_pred = np.concatenate(oof_pred)\n",
    "    return y_true, y_proba, y_pred\n",
    "\n",
    "y_true, y_proba, y_pred = oof_fixed_window(\n",
    "    feat_df, feature_cols,\n",
    "    test_size=7,\n",
    "    train_window=20,\n",
    "    val_size=5,\n",
    "    step_size=7\n",
    ")\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "try:\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "except Exception:\n",
    "    auc = np.nan\n",
    "\n",
    "print(f\"OOF Accuracy={acc:.3f}, BalancedAcc={bacc:.3f}, AUC={auc:.3f}, N={len(y_true)}\")\n",
    "\n",
    "# Approximate 95% CI for accuracy (normal approximation)\n",
    "from math import sqrt\n",
    "n = len(y_true)\n",
    "phat = acc\n",
    "z = 1.96\n",
    "half = z*np.sqrt(phat*(1-phat)/n)\n",
    "print(f\"Acc 95% CI (approx): [{phat - half:.3f}, {phat + half:.3f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cwq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
